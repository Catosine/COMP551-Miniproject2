{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP551 Mini Project 2 - IMDB Sentiment Analysis  \n",
    "This is the codes of mini project2 - IMDB Sentiment Analysis.  \n",
    "\n",
    "## AUTHORS\n",
    "Pengnan Fan, ID#260768510  \n",
    "Qifei Zhao, ID#260719382  \n",
    "\n",
    "## TASKS\n",
    "1. **Bernoulli Naive Bayes** (w/o any external library).  \n",
    "2. **At least 2** out of 3 classifiers from the SciKit. i.e. suggestions: logistic regression, decision tree, or support vector machines  \n",
    "3. **At least 2** different features extraction pipelines for processing the data. \n",
    "4. A model validation. i.e. **K-fold cross validation**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Preparation  \n",
    "by Pengnan, Sherry, and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.1 - List of Packages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import numpy\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import contractions\n",
    "from itertools import groupby\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.2 - Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Please add your address here as string\n",
    "ADDRESS_TRAIN_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "ADDRESS_TEST_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan Fan\n",
    "# @param address of train set\n",
    "# @return a dict of list of dict\n",
    "def loadData(address):\n",
    "    print(\"Start loading negative set\")\n",
    "    neg = sklearn.datasets.load_files(address, categories={\"neg\"})\n",
    "    print(\"Complete loading negative set\")\n",
    "    print(\"Start loading positive set\")\n",
    "    pos = sklearn.datasets.load_files(address, categories={\"pos\"})\n",
    "    print(\"Complete loading postive set\")\n",
    "    \n",
    "    negSet = list()\n",
    "    count = 0\n",
    "    size = len(neg.data)\n",
    "    for x in neg.data:\n",
    "        negSet.append({\"comment\":x.decode('utf-8'), \"isPos\":0})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Preparing negative set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    posSet = list()\n",
    "    count = 0\n",
    "    size = len(pos.data)\n",
    "    for x in pos.data:\n",
    "        posSet.append({\"comment\":x.decode('utf-8'), \"isPos\":1})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Prepare positive set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    output = {'pos':posSet, 'neg':negSet, 'all':posSet+negSet}\n",
    "    print(\"\\nFinish preparing\")\n",
    "    return output\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Path\n",
    "train_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "test_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads train data to a list\n",
    "# @author\n",
    "# @param class_name: either \"pos\" for loading positive data and otherwise loading negative data\n",
    "# @return data: list of train data\n",
    "def load_train(class_name):\n",
    "    label=0\n",
    "    if class_name is \"pos\":\n",
    "        print(\"Start loading positive data\")\n",
    "        label=1\n",
    "    else:\n",
    "        print(\"Start loading negative data\")\n",
    "    data=[]\n",
    "    count = 0\n",
    "    lenW = 12500\n",
    "    for file in glob.glob(train_path+\"/\"+class_name+\"/*.txt\"):\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),label])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    if class_name is \"pos\":\n",
    "        print(\"\\nComplete loading positive data\")\n",
    "    else:\n",
    "        print(\"\\nComplete loading negative data\")\n",
    "    return data\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads test data to a list\n",
    "# @author\n",
    "# @return data: list of test data\n",
    "def load_test():\n",
    "    data=[]\n",
    "    print(\"Start loading test data\")\n",
    "    count = 0\n",
    "    lenW = 25000\n",
    "    for i in range(0,25000):\n",
    "        file=test_path+\"/\"+str(i)+\".txt\"\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),i])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete loading test data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set for prediction\n",
    "# @param wordExistance: dict {'pos' -> pos word existance, 'neg' -> neg word existance, 'all' -> all word existance}\n",
    "# @param wordSet: list of unique words\n",
    "# @param size: dict {'pos' -> pos size, 'neg' -> neg size, 'all' -> all size}\n",
    "# @return prediction\n",
    "def bernoulliNaiveBayes(dataSet, wordExistance, wordSet, size):\n",
    "    pPos = size['pos']/size['all']\n",
    "    pNeg = size['neg']/size['all']\n",
    "    prediction = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start Bernoulli naive Bayes classifying\")\n",
    "    for exp in dataSet:\n",
    "        #pPos_x = numpy.log([pPos])\n",
    "        #pNeg_x = numpy.log([pNeg])\n",
    "        \n",
    "        pred = numpy.log([pPos/nNeg])\n",
    "        \n",
    "        # Calculating P(Y|X)\n",
    "        for word in wordSet:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            \n",
    "            if word in exp:\n",
    "                pos = (wordExistance['pos'][word]+1)/(size['pos']+2)\n",
    "                neg = (wordExistance['neg'][word]+1)/(size['neg']+2)\n",
    "            else:\n",
    "                pos = 1 - ((wordExistance['pos'][word]+1)/(size['pos']+2))\n",
    "                neg = 1 - ((wordExistance['neg'][word]+1)/(size['neg']+2))\n",
    "            \n",
    "            #pPos_x += numpy.log([pos])\n",
    "            #pNeg_x += numpy.log([neg])\n",
    "            pred+=numpy.log([pos])\n",
    "        \n",
    "        # Logistic decision boundary\n",
    "        #log_ratio = pPos_x - pNeg_x\n",
    "        \n",
    "        if pred > 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print(\"\\nComplete Bernoulli naive Bayes classifying\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: train set with label\n",
    "# @param prediction: prediction of each example in the trainSet\n",
    "# @return result: list of results: {TP: true pos, TN: true neg, FP: false pos, FN: false neg}\n",
    "def evaluation(dataSet, prediction):\n",
    "    size = len(prediction)\n",
    "    count = 0\n",
    "    result = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    \n",
    "    print(\"Start evaluating classification\")\n",
    "    for i in range(size):\n",
    "        if prediction[i]==1:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['TP']+=1\n",
    "            else:\n",
    "                result['FP']+=1\n",
    "        else:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['FN']+=1\n",
    "            else:\n",
    "                result['TN']+=1\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete evaluating classification\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Logistic Regression, Support Vector Machine, and Decision Tree  \n",
    "by Sherry and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_data, 0.8, 0.2, True, False, 1, 3, 100)\n",
    "def LogReg(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN, c):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "    lg=LogisticRegression(C=c)\n",
    "    start_learn = time.time()\n",
    "    lg.fit(X,train_y)\n",
    "    end_learn = time.time()\n",
    "    #result = sklearn.model_selection.cross_validate(lg, X, y_train, cv=5, return_train_score=True)\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred = lg.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"LR Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score\n",
    "    \n",
    "    '''\n",
    "    print (\"LR Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"LR Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN, c):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    clf=SVC(kernel='linear', C=c)\n",
    "    start_learn = time.time()\n",
    "    clf.fit(X,train_y)\n",
    "    end_learn = time.time()\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred=clf.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"SVM Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score\n",
    "    #result = sklearn.model_selection.cross_validate(clf, X, y_train, cv=5, return_train_score=True)\n",
    "    '''\n",
    "    print (\"SVM Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"SVM Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecTree(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "     \n",
    "\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=2,max_leaf_nodes=300,min_samples_split=2)\n",
    "    start_learn = time.time()\n",
    "    dt.fit(X, train_y)\n",
    "    end_learn = time.time()\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred = dt.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"Decision Tree Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Feature Extraction Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 - Word Frequency without Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @acknowledgement Yuxiang Ma, for this function is edited based on his in miniproject1\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: word frequency without stopwords\n",
    "def wordsFrequencyStopword(dataSet):\n",
    "    stopwordCount = dict()\n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of positive set\")\n",
    "    \n",
    "    for comment in dataSet['pos']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    posCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of positive set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of negative set\")\n",
    "    \n",
    "    for comment in dataSet['neg']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    negCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of negative set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of all set\")\n",
    "    \n",
    "    for comment in dataSet['all']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    allCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of all set\")\n",
    "    \n",
    "    stopwordCount = {'pos':posCountDict, 'neg':negCountDict, 'all':allCountDict}\n",
    "    \n",
    "    return stopwordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 - Number of Naive Existance of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of words of num of naive existances\n",
    "def numOfExistanceNaive(dataSet):\n",
    "    naiveCount = dict()\n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of positive set\")\n",
    "    for comment in dataSet['pos']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    posCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of positive set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of negative set\")\n",
    "    for comment in dataSet['neg']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    negCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of negative set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of all set\")\n",
    "    for comment in dataSet['all']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    allCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of all set\")\n",
    "    \n",
    "    naiveCount = {'pos':posCount, 'neg':negCount, 'all':allCount}\n",
    "    \n",
    "    return naiveCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.1 - Number of Existance of Words without Stopwords, Duplicates, and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords, duplicates, and with stemmer\n",
    "def advancedNumOfExistance(dataSet):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.2 - With Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords and with stemmer\n",
    "def advancedNumOfExistanceWithDuplicates(dataSet):\n",
    "    output = dict()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    eachFreqPos = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_pos = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqPos.append(temp_pos)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    eachFreqNeg = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "                \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_neg = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqNeg.append(temp_neg)\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    eachFreq = eachFreqPos+eachFreqNeg\n",
    "    \n",
    "    output = {'allFreq':countDict, 'individual':eachFreq}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.3 - Number of Existance of N-gram Words without Stopwords, Duplicates and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @param n: used for n-gram\n",
    "# @return countDict: list of num of existances of words without stopwords, duplicates and with stemmer\n",
    "def nGram(dataSet, n):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nCounting counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.4 - CountAllWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set used to learn\n",
    "# @return output: list of unique words shown in dataSet\n",
    "def allWords(dataSet):\n",
    "    output = list()\n",
    "    lenW = len(dataSet)\n",
    "    count = 0\n",
    "    print(\"Start counting all words\")\n",
    "    for comments in dataSet:\n",
    "        for word in comments:\n",
    "            if word not in output:\n",
    "                output.append(word)\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting all words\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4 - Data Standardization and Clean-up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet:comment data need to be proceed\n",
    "# @return proceedData\n",
    "def dataStandardization(dataSet):\n",
    "    proceedData = list()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    \n",
    "    print(\"Start proceeding data\")\n",
    "    for data in dataSet: \n",
    "        comment = contractions.fix(data['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        toAdd = list()\n",
    "        for x in processedComment:\n",
    "            if x not in stopwordsSet and x not in toAdd:\n",
    "                toAdd.append(x)\n",
    "                \n",
    "        proceedData.append(toAdd)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete proceeding data\")\n",
    "    return proceedData\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "# @param dataSet:set of data with comments\n",
    "# @return extendedSentence:set of comments without abbr. i.e. it's -> it is\n",
    "def extendSentences(dataSet):\n",
    "    extenedSentence = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start extending sentences\")\n",
    "    for data in dataSet:\n",
    "        extenedSentence.append([contractions.fix(data[0].lower()),data[1]])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete extending sentences\")\n",
    "    return extenedSentence\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "def removePunctuationAndStopwords(dataSet, additionalStopwords=[]):\n",
    "    clean = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    print(\"Start removing punctuation and stopwords\")\n",
    "    for data in dataSet:\n",
    "        comment = data[0].lower()\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        fixed = str()\n",
    "        for word in processedComment:\n",
    "            if word not in stopwordsSet and word not in additionalStopwords:\n",
    "                fixed = fixed + \" \" + word\n",
    "        clean.append([fixed,data[1]])\n",
    "    \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing punctuation and stopwords\")\n",
    "    return clean\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def lemmatize(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start lemmatizing\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        clean_text = str()\n",
    "        for word in words:\n",
    "            w = lemm.lemmatize(word)\n",
    "            clean_text += w + \" \"\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete lemmatizing\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def removeDuplicates(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start removing duplicates\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        noDup = list()\n",
    "        for word in words:\n",
    "            if word not in noDup:\n",
    "                noDup.append(word)\n",
    "                \n",
    "        clean_text = str()\n",
    "        for w in noDup:\n",
    "            clean_text = clean_text + \" \" + w\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing duplicates\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Qifei, Pengnan\n",
    "# @param text: a string of comment\n",
    "# @param custom_stopwords:list of additional stopwords\n",
    "# @return clean_txt: a string of clean-up comment\n",
    "def cleanUp(text, custom_stopwords=[]):\n",
    "    # Initilaise Lemmatizer object:\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Extend words\n",
    "    comment = contractions.fix(text.lower())\n",
    "    sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "    sent_clean = comment.translate(sent_map)\n",
    "    \n",
    "    # Load NLTK stopwords:\n",
    "    my_stopwords = stopwords.words('english') + custom_stopwords\n",
    "    \n",
    "    clean_text = ''\n",
    "    \n",
    "    words = word_tokenize(sent_clean) # word_tokenize() takes care of stripping too.\n",
    "    \n",
    "    for word in words:\n",
    "        w = lemm.lemmatize(w)\n",
    "        if w not in my_stopwords and len(w)>2:\n",
    "            clean_text += w + \" \"\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author\n",
    "# @param dataset: list of comments\n",
    "def cleanUp_data(dataset):\n",
    "    for i in range(0,len(dataset)):\n",
    "        comment=cleanUp(dataset[i][0])\n",
    "        dataset[i][0]=comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(mode, X,y_train, fold, c, train_score):\n",
    "    if mode is \"LR\":\n",
    "        lg=LogisticRegression(C=c)\n",
    "        lg.fit(X,y_train)\n",
    "        result = sklearn.model_selection.cross_validate(lg, X,y_train, cv=fold, return_train_score=train_score)\n",
    "        print (\"Logistic Regression Train Accuracy = \", result['train_score'])\n",
    "        print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "        print (\"Logistic Regression Cross Validation Accuracy = \", result['test_score'])\n",
    "        print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "        return result\n",
    "    \n",
    "    elif mode is \"SVM\":\n",
    "        clf=SVC(kernel='linear', C=c)\n",
    "        clf.fit(X,y_train)\n",
    "        result = sklearn.model_selection.cross_validate(clf, X, y_train, cv=fold, return_train_score=train_score)\n",
    "        print (\"SVM Train Accuracy = \", result['train_score'])\n",
    "        print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "        print (\"SVM Cross Validation Accuracy = \", result['test_score'])\n",
    "        print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "        return result\n",
    "    elif mode is \"DT\":\n",
    "        dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=2,max_leaf_nodes=300,min_samples_split=2)\n",
    "        dt.fit(X, y_train)\n",
    "        result = sklearn.model_selection.cross_validate(dt, X, y_train, cv=fold, return_train_score=train_score)\n",
    "    else:\n",
    "        print(\"INVALID INPUTS\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading positive data\n",
      "[=================== ] 99%\n",
      "Complete loading positive data\n",
      "Start loading negative data\n",
      "[=================== ] 99%\n",
      "Complete loading negative data\n",
      "Start loading test data\n",
      "[=================== ] 99%\n",
      "Complete loading test data\n"
     ]
    }
   ],
   "source": [
    "train_data=load_train(\"pos\")+load_train(\"neg\")\n",
    "test_data=load_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selecting Model Based on Accuracy and Runing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy:  0.8502\n",
      "Learning Running Time:  1.4989209175109863 seconds\n",
      "Predicting Running Time:  0.004904508590698242 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8502"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg(train_data, 0.8, 0.2, True, False, 1, 1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.8812\n",
      "Learning Running Time:  341.59919571876526 seconds\n",
      "Predicting Running Time:  84.45325541496277 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8812"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM(train_data, 0.8, 0.2, True, False, 1, 1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.6848\n",
      "Learning Running Time:  10.170814037322998 seconds\n",
      "Predicting Running Time:  0.006982088088989258 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6848"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecTree(train_data, 0.8, 0.2, True, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectRegularizationParameter(mode, dataSet, minN, maxN, cv, tfidf, maxC):\n",
    "    train_x=[comment[0] for comment in dataSet]\n",
    "    train_y=[comment[1] for comment in dataSet]\n",
    "    output = list()\n",
    "    bestScore = 0\n",
    "    bestParameter = 0\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    for i in range(maxC):\n",
    "        print(\"< Cross Validation\", i+1, \">\")\n",
    "        result = crossValidation(mode, X,train_y, cv, 1+i, True)\n",
    "        score = numpy.mean(result['test_score'])\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            bestParameter = i\n",
    "        output.append([score])\n",
    "    print(\"Best Parameter: \", bestParameter,\", Best Score: \", bestScore)\n",
    "    return [output, bestParameter, bestScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Cross Validation 1 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.87515 0.87805 0.87735 0.87565 0.87645]\n",
      "Avg Train Accuracy =  0.87653\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8506 0.8454 0.8444 0.844  0.8518]\n",
      "Avg Validation Accuracy =  0.84724\n",
      "< Cross Validation 2 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.89595 0.89745 0.8969  0.896   0.8949 ]\n",
      "Avg Train Accuracy =  0.89624\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8628 0.856  0.8554 0.855  0.8608]\n",
      "Avg Validation Accuracy =  0.858\n",
      "< Cross Validation 3 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9071  0.9084  0.9088  0.90645 0.9058 ]\n",
      "Avg Train Accuracy =  0.9073100000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8676 0.8616 0.8556 0.8588 0.8662]\n",
      "Avg Validation Accuracy =  0.8619600000000001\n",
      "< Cross Validation 4 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.91465 0.9166  0.91635 0.91425 0.9131 ]\n",
      "Avg Train Accuracy =  0.9149899999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8692 0.8634 0.8572 0.8632 0.8684]\n",
      "Avg Validation Accuracy =  0.8642799999999999\n",
      "< Cross Validation 5 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9209  0.92335 0.92275 0.92015 0.91845]\n",
      "Avg Train Accuracy =  0.9211200000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8696 0.8636 0.8584 0.8642 0.871 ]\n",
      "Avg Validation Accuracy =  0.8653600000000001\n",
      "< Cross Validation 6 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.92615 0.92875 0.928   0.92595 0.9247 ]\n",
      "Avg Train Accuracy =  0.9267099999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8698 0.8614 0.8586 0.8632 0.8726]\n",
      "Avg Validation Accuracy =  0.8651200000000001\n",
      "< Cross Validation 7 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9303  0.9319  0.932   0.92965 0.92875]\n",
      "Avg Train Accuracy =  0.9305199999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8706 0.8612 0.8586 0.8634 0.872 ]\n",
      "Avg Validation Accuracy =  0.86516\n",
      "< Cross Validation 8 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9336  0.93535 0.9354  0.9334  0.9321 ]\n",
      "Avg Train Accuracy =  0.9339700000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8716 0.861  0.858  0.8648 0.8722]\n",
      "Avg Validation Accuracy =  0.8655200000000001\n",
      "< Cross Validation 9 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9371  0.93825 0.9384  0.9366  0.93505]\n",
      "Avg Train Accuracy =  0.9370800000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.871  0.861  0.8568 0.8644 0.8738]\n",
      "Avg Validation Accuracy =  0.8654\n",
      "< Cross Validation 10 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.93995 0.941   0.9409  0.9396  0.9383 ]\n",
      "Avg Train Accuracy =  0.93995\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8714 0.8582 0.8566 0.8636 0.8744]\n",
      "Avg Validation Accuracy =  0.8648399999999998\n",
      "< Cross Validation 11 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.94195 0.94435 0.94285 0.9428  0.94075]\n",
      "Avg Train Accuracy =  0.9425399999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.872  0.8576 0.856  0.8634 0.8744]\n",
      "Avg Validation Accuracy =  0.8646799999999999\n",
      "< Cross Validation 12 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.94445 0.9472  0.9452  0.9453  0.94335]\n",
      "Avg Train Accuracy =  0.9451\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8728 0.8574 0.855  0.8632 0.8736]\n",
      "Avg Validation Accuracy =  0.8644000000000001\n",
      "< Cross Validation 13 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.94605 0.9494  0.9474  0.94705 0.9459 ]\n",
      "Avg Train Accuracy =  0.9471599999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8722 0.8566 0.8546 0.8632 0.8732]\n",
      "Avg Validation Accuracy =  0.86396\n",
      "< Cross Validation 14 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.94755 0.95115 0.94985 0.9487  0.948  ]\n",
      "Avg Train Accuracy =  0.9490500000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8726 0.8546 0.8538 0.8626 0.8726]\n",
      "Avg Validation Accuracy =  0.86324\n",
      "< Cross Validation 15 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9493  0.95285 0.95165 0.95055 0.9497 ]\n",
      "Avg Train Accuracy =  0.95081\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8724 0.8536 0.8536 0.862  0.8742]\n",
      "Avg Validation Accuracy =  0.86316\n",
      "< Cross Validation 16 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9509  0.9544  0.95345 0.9519  0.951  ]\n",
      "Avg Train Accuracy =  0.9523299999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8724 0.8534 0.8538 0.8622 0.8746]\n",
      "Avg Validation Accuracy =  0.8632799999999999\n",
      "< Cross Validation 17 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.95195 0.95595 0.95455 0.9532  0.95265]\n",
      "Avg Train Accuracy =  0.95366\n",
      "Logistic Regression Cross Validation Accuracy =  [0.872  0.8528 0.853  0.8622 0.8744]\n",
      "Avg Validation Accuracy =  0.86288\n",
      "< Cross Validation 18 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.95335 0.95745 0.9561  0.95455 0.95455]\n",
      "Avg Train Accuracy =  0.9552000000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8718 0.8518 0.8528 0.8624 0.8746]\n",
      "Avg Validation Accuracy =  0.8626799999999999\n",
      "< Cross Validation 19 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9546  0.9593  0.95755 0.95595 0.95565]\n",
      "Avg Train Accuracy =  0.9566100000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8722 0.8512 0.853  0.8624 0.8742]\n",
      "Avg Validation Accuracy =  0.8625999999999999\n",
      "< Cross Validation 20 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9561  0.96045 0.95895 0.95725 0.9568 ]\n",
      "Avg Train Accuracy =  0.95791\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8722 0.8512 0.8532 0.8624 0.8742]\n",
      "Avg Validation Accuracy =  0.8626400000000001\n",
      "< Cross Validation 21 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.95765 0.96185 0.9602  0.9588  0.9586 ]\n",
      "Avg Train Accuracy =  0.95942\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8722 0.8508 0.8526 0.8626 0.8742]\n",
      "Avg Validation Accuracy =  0.8624799999999999\n",
      "< Cross Validation 22 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.95885 0.96295 0.96115 0.95965 0.9596 ]\n",
      "Avg Train Accuracy =  0.96044\n",
      "Logistic Regression Cross Validation Accuracy =  [0.872  0.85   0.8526 0.8624 0.8744]\n",
      "Avg Validation Accuracy =  0.8622799999999999\n",
      "< Cross Validation 23 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9597  0.9638  0.96275 0.9607  0.9611 ]\n",
      "Avg Train Accuracy =  0.96161\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8716 0.8498 0.8522 0.8626 0.8748]\n",
      "Avg Validation Accuracy =  0.8622\n",
      "< Cross Validation 24 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9607  0.9646  0.96385 0.96175 0.962  ]\n",
      "Avg Train Accuracy =  0.96258\n",
      "Logistic Regression Cross Validation Accuracy =  [0.871  0.849  0.8516 0.8626 0.8746]\n",
      "Avg Validation Accuracy =  0.86176\n",
      "< Cross Validation 25 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96135 0.96575 0.9649  0.9627  0.96315]\n",
      "Avg Train Accuracy =  0.96357\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8706 0.8484 0.8516 0.8622 0.8736]\n",
      "Avg Validation Accuracy =  0.86128\n",
      "< Cross Validation 26 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96235 0.9666  0.9655  0.96365 0.96425]\n",
      "Avg Train Accuracy =  0.96447\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8702 0.8476 0.8514 0.8618 0.873 ]\n",
      "Avg Validation Accuracy =  0.8608\n",
      "< Cross Validation 27 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9631  0.9676  0.9664  0.9647  0.96485]\n",
      "Avg Train Accuracy =  0.96533\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8464 0.8502 0.8618 0.873 ]\n",
      "Avg Validation Accuracy =  0.86028\n",
      "< Cross Validation 28 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96375 0.9686  0.96735 0.9656  0.96595]\n",
      "Avg Train Accuracy =  0.9662500000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8464 0.85   0.8616 0.873 ]\n",
      "Avg Validation Accuracy =  0.8602000000000001\n",
      "< Cross Validation 29 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96475 0.9692  0.9678  0.96655 0.96735]\n",
      "Avg Train Accuracy =  0.9671299999999998\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8454 0.8502 0.862  0.8724]\n",
      "Avg Validation Accuracy =  0.86\n",
      "< Cross Validation 30 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96515 0.96975 0.9685  0.9672  0.968  ]\n",
      "Avg Train Accuracy =  0.9677199999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8442 0.8496 0.8622 0.8724]\n",
      "Avg Validation Accuracy =  0.85968\n",
      "< Cross Validation 31 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9661  0.9707  0.96915 0.96795 0.96895]\n",
      "Avg Train Accuracy =  0.96857\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8698 0.8448 0.8498 0.8622 0.8714]\n",
      "Avg Validation Accuracy =  0.8596\n",
      "< Cross Validation 32 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9669  0.97135 0.97015 0.9686  0.9698 ]\n",
      "Avg Train Accuracy =  0.96936\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8698 0.8434 0.8494 0.8618 0.8712]\n",
      "Avg Validation Accuracy =  0.8591200000000001\n",
      "< Cross Validation 33 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96745 0.97225 0.9706  0.96925 0.97025]\n",
      "Avg Train Accuracy =  0.96996\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8428 0.8494 0.8622 0.8714]\n",
      "Avg Validation Accuracy =  0.8591599999999999\n",
      "< Cross Validation 34 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96815 0.9734  0.97125 0.97005 0.971  ]\n",
      "Avg Train Accuracy =  0.9707699999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.87   0.8418 0.8492 0.862  0.871 ]\n",
      "Avg Validation Accuracy =  0.8588000000000001\n",
      "< Cross Validation 35 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.96865 0.974   0.9722  0.9708  0.97175]\n",
      "Avg Train Accuracy =  0.97148\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8696 0.8422 0.849  0.8614 0.8708]\n",
      "Avg Validation Accuracy =  0.8586\n",
      "< Cross Validation 36 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9695  0.97465 0.97275 0.9714  0.9722 ]\n",
      "Avg Train Accuracy =  0.9721\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8696 0.8412 0.849  0.8616 0.8706]\n",
      "Avg Validation Accuracy =  0.8583999999999999\n",
      "< Cross Validation 37 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9705  0.9753  0.9732  0.97235 0.9729 ]\n",
      "Avg Train Accuracy =  0.97285\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8694 0.8414 0.849  0.8618 0.8708]\n",
      "Avg Validation Accuracy =  0.8584800000000001\n",
      "< Cross Validation 38 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9712  0.9758  0.97415 0.9729  0.9735 ]\n",
      "Avg Train Accuracy =  0.9735099999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8694 0.8414 0.8484 0.8616 0.871 ]\n",
      "Avg Validation Accuracy =  0.85836\n",
      "< Cross Validation 39 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9719  0.9763  0.9745  0.97345 0.974  ]\n",
      "Avg Train Accuracy =  0.97403\n",
      "Logistic Regression Cross Validation Accuracy =  [0.869  0.841  0.848  0.8612 0.8704]\n",
      "Avg Validation Accuracy =  0.85792\n",
      "< Cross Validation 40 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97265 0.9767  0.97495 0.97415 0.97445]\n",
      "Avg Train Accuracy =  0.9745799999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8688 0.841  0.8478 0.861  0.8698]\n",
      "Avg Validation Accuracy =  0.8576799999999999\n",
      "< Cross Validation 41 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97315 0.9775  0.97575 0.9745  0.9748 ]\n",
      "Avg Train Accuracy =  0.97514\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8684 0.8406 0.8482 0.8608 0.869 ]\n",
      "Avg Validation Accuracy =  0.8573999999999999\n",
      "< Cross Validation 42 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9739  0.9779  0.97635 0.975   0.9756 ]\n",
      "Avg Train Accuracy =  0.97575\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8684 0.8404 0.848  0.8608 0.8692]\n",
      "Avg Validation Accuracy =  0.8573600000000001\n",
      "< Cross Validation 43 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97435 0.97825 0.97695 0.97575 0.9763 ]\n",
      "Avg Train Accuracy =  0.97632\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8684 0.8396 0.8476 0.8608 0.8684]\n",
      "Avg Validation Accuracy =  0.8569600000000002\n",
      "< Cross Validation 44 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9749  0.9789  0.97745 0.97625 0.97675]\n",
      "Avg Train Accuracy =  0.97685\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8682 0.8396 0.8472 0.8606 0.8684]\n",
      "Avg Validation Accuracy =  0.8568\n",
      "< Cross Validation 45 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9755 0.9791 0.9779 0.9767 0.9771]\n",
      "Avg Train Accuracy =  0.97726\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8684 0.8396 0.8466 0.8612 0.8682]\n",
      "Avg Validation Accuracy =  0.8568\n",
      "< Cross Validation 46 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.976   0.9797  0.97845 0.9771  0.97735]\n",
      "Avg Train Accuracy =  0.97772\n",
      "Logistic Regression Cross Validation Accuracy =  [0.868  0.8394 0.8464 0.8612 0.8684]\n",
      "Avg Validation Accuracy =  0.8566800000000001\n",
      "< Cross Validation 47 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97635 0.98005 0.9789  0.9775  0.9778 ]\n",
      "Avg Train Accuracy =  0.97812\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8676 0.8392 0.8468 0.8612 0.8682]\n",
      "Avg Validation Accuracy =  0.8565999999999999\n",
      "< Cross Validation 48 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9766  0.9806  0.97945 0.9782  0.9781 ]\n",
      "Avg Train Accuracy =  0.9785900000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.868  0.839  0.8466 0.8606 0.8674]\n",
      "Avg Validation Accuracy =  0.85632\n",
      "< Cross Validation 49 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.977   0.9809  0.9798  0.97835 0.97855]\n",
      "Avg Train Accuracy =  0.9789200000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8682 0.8396 0.8466 0.86   0.8674]\n",
      "Avg Validation Accuracy =  0.8563600000000001\n",
      "< Cross Validation 50 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97775 0.9814  0.98    0.97915 0.97895]\n",
      "Avg Train Accuracy =  0.9794500000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8682 0.84   0.8464 0.8598 0.8678]\n",
      "Avg Validation Accuracy =  0.8564399999999999\n",
      "< Cross Validation 51 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97815 0.9817  0.9803  0.97935 0.9794 ]\n",
      "Avg Train Accuracy =  0.9797800000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8676 0.8394 0.8464 0.86   0.8674]\n",
      "Avg Validation Accuracy =  0.8561599999999998\n",
      "< Cross Validation 52 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97855 0.982   0.9807  0.97985 0.9797 ]\n",
      "Avg Train Accuracy =  0.98016\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8672 0.8394 0.846  0.8596 0.8672]\n",
      "Avg Validation Accuracy =  0.85588\n",
      "< Cross Validation 53 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97895 0.98245 0.9811  0.9803  0.98   ]\n",
      "Avg Train Accuracy =  0.9805599999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8668 0.8394 0.8462 0.8592 0.8672]\n",
      "Avg Validation Accuracy =  0.8557600000000001\n",
      "< Cross Validation 54 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.97935 0.9828  0.9816  0.9808  0.9804 ]\n",
      "Avg Train Accuracy =  0.98099\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8664 0.8394 0.8462 0.8592 0.8664]\n",
      "Avg Validation Accuracy =  0.85552\n",
      "< Cross Validation 55 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9796  0.9833  0.98205 0.98115 0.9809 ]\n",
      "Avg Train Accuracy =  0.9814\n",
      "Logistic Regression Cross Validation Accuracy =  [0.866  0.8396 0.8458 0.8588 0.8658]\n",
      "Avg Validation Accuracy =  0.8552\n",
      "< Cross Validation 56 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98025 0.9837  0.98255 0.98145 0.9813 ]\n",
      "Avg Train Accuracy =  0.98185\n",
      "Logistic Regression Cross Validation Accuracy =  [0.866  0.8396 0.8456 0.8588 0.8654]\n",
      "Avg Validation Accuracy =  0.8550800000000001\n",
      "< Cross Validation 57 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9807  0.9841  0.98285 0.98185 0.98155]\n",
      "Avg Train Accuracy =  0.98221\n",
      "Logistic Regression Cross Validation Accuracy =  [0.866  0.8394 0.8456 0.859  0.8652]\n",
      "Avg Validation Accuracy =  0.85504\n",
      "< Cross Validation 58 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98095 0.9845  0.98305 0.98215 0.982  ]\n",
      "Avg Train Accuracy =  0.98253\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8658 0.8392 0.845  0.859  0.8654]\n",
      "Avg Validation Accuracy =  0.85488\n",
      "< Cross Validation 59 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9811  0.9846  0.9833  0.9825  0.98235]\n",
      "Avg Train Accuracy =  0.98277\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8654 0.8392 0.8454 0.8592 0.8648]\n",
      "Avg Validation Accuracy =  0.8548\n",
      "< Cross Validation 60 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9814  0.98505 0.98355 0.9827  0.9825 ]\n",
      "Avg Train Accuracy =  0.9830400000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8654 0.839  0.8456 0.859  0.8646]\n",
      "Avg Validation Accuracy =  0.85472\n",
      "< Cross Validation 61 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9818  0.9853  0.9839  0.98295 0.98275]\n",
      "Avg Train Accuracy =  0.9833399999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8652 0.839  0.8456 0.8586 0.8646]\n",
      "Avg Validation Accuracy =  0.8545999999999999\n",
      "< Cross Validation 62 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9822  0.98545 0.9843  0.98325 0.9831 ]\n",
      "Avg Train Accuracy =  0.9836600000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8648 0.8384 0.8454 0.8586 0.8652]\n",
      "Avg Validation Accuracy =  0.85448\n",
      "< Cross Validation 63 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98265 0.9856  0.98445 0.9835  0.98325]\n",
      "Avg Train Accuracy =  0.98389\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8646 0.838  0.846  0.8586 0.8652]\n",
      "Avg Validation Accuracy =  0.85448\n",
      "< Cross Validation 64 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98305 0.9857  0.98475 0.9838  0.9834 ]\n",
      "Avg Train Accuracy =  0.98414\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8648 0.8378 0.8456 0.859  0.8652]\n",
      "Avg Validation Accuracy =  0.85448\n",
      "< Cross Validation 65 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9832  0.9859  0.98495 0.9841  0.9837 ]\n",
      "Avg Train Accuracy =  0.98437\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8646 0.838  0.8456 0.859  0.865 ]\n",
      "Avg Validation Accuracy =  0.85444\n",
      "< Cross Validation 66 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9834  0.9863  0.98505 0.98435 0.984  ]\n",
      "Avg Train Accuracy =  0.9846199999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8644 0.8382 0.8454 0.859  0.8648]\n",
      "Avg Validation Accuracy =  0.85436\n",
      "< Cross Validation 67 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98385 0.9866  0.98525 0.98475 0.9843 ]\n",
      "Avg Train Accuracy =  0.9849500000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.864  0.838  0.8452 0.859  0.8648]\n",
      "Avg Validation Accuracy =  0.8542\n",
      "< Cross Validation 68 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98395 0.98665 0.98525 0.985   0.98455]\n",
      "Avg Train Accuracy =  0.98508\n",
      "Logistic Regression Cross Validation Accuracy =  [0.864  0.838  0.8446 0.8592 0.8648]\n",
      "Avg Validation Accuracy =  0.85412\n",
      "< Cross Validation 69 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98415 0.98695 0.98565 0.98525 0.9847 ]\n",
      "Avg Train Accuracy =  0.9853400000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.864  0.8372 0.8442 0.8594 0.8652]\n",
      "Avg Validation Accuracy =  0.8539999999999999\n",
      "< Cross Validation 70 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98455 0.98715 0.986   0.98555 0.9851 ]\n",
      "Avg Train Accuracy =  0.98567\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8636 0.8368 0.8444 0.859  0.8654]\n",
      "Avg Validation Accuracy =  0.8538400000000002\n",
      "< Cross Validation 71 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9847  0.98735 0.98625 0.98575 0.98535]\n",
      "Avg Train Accuracy =  0.9858799999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8634 0.8368 0.8444 0.8588 0.865 ]\n",
      "Avg Validation Accuracy =  0.85368\n",
      "< Cross Validation 72 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98495 0.9876  0.98655 0.98605 0.9856 ]\n",
      "Avg Train Accuracy =  0.9861500000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8636 0.8368 0.8442 0.859  0.8648]\n",
      "Avg Validation Accuracy =  0.85368\n",
      "< Cross Validation 73 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9853  0.98775 0.9868  0.9862  0.98595]\n",
      "Avg Train Accuracy =  0.9863999999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.864  0.8366 0.8442 0.859  0.8642]\n",
      "Avg Validation Accuracy =  0.8535999999999999\n",
      "< Cross Validation 74 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98555 0.98785 0.9869  0.98645 0.9861 ]\n",
      "Avg Train Accuracy =  0.9865700000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8636 0.8364 0.8438 0.8592 0.8642]\n",
      "Avg Validation Accuracy =  0.85344\n",
      "< Cross Validation 75 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98565 0.98825 0.9872  0.9866  0.9864 ]\n",
      "Avg Train Accuracy =  0.98682\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8634 0.836  0.844  0.8592 0.8642]\n",
      "Avg Validation Accuracy =  0.85336\n",
      "< Cross Validation 76 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.98585 0.9884  0.9874  0.9869  0.98655]\n",
      "Avg Train Accuracy =  0.98702\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8632 0.8358 0.8438 0.859  0.8646]\n",
      "Avg Validation Accuracy =  0.85328\n",
      "< Cross Validation 77 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-15c573be2d91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselectRegularizationParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LR\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-db5a63e52499>\u001b[0m in \u001b[0;36mselectRegularizationParameter\u001b[1;34m(mode, dataSet, minN, maxN, cv, tfidf, maxC)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"< Cross Validation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\">\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossValidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbestScore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-33b01666cb81>\u001b[0m in \u001b[0;36mcrossValidation\u001b[1;34m(mode, X, y_train, fold, c, train_score)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mlg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression Train Accuracy = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avg Train Accuracy = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1303\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameter = selectRegularizationParameter(\"LR\", train_data, 1, 1, 5, False, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Cross Validation 1 >\n"
     ]
    }
   ],
   "source": [
    "selectRegularizationParameter(\"SVM\", train_data, 1, 1, 5, False, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.87515 0.87805 0.87735 0.87565 0.87645]\n",
      "Avg Train Accuracy =  0.87653\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8506 0.8454 0.8444 0.844  0.8518]\n",
      "Avg Validation Accuracy =  0.84724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.99115705, 0.99589705, 1.1870811 , 0.99713731, 0.96800184]),\n",
       " 'score_time': array([0.00500417, 0.00402164, 0.00398922, 0.00398922, 0.00399065]),\n",
       " 'test_score': array([0.8506, 0.8454, 0.8444, 0.844 , 0.8518]),\n",
       " 'train_score': array([0.87515, 0.87805, 0.87735, 0.87565, 0.87645])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No data processing Test, Unigram\")\n",
    "crossValidation(\"LR\", X,train_y, 5, 1.0, True)\n",
    "#LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValidation(\"LR\", X,train_y, 5, 1.0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9329  0.9363  0.93415 0.9335  0.9337 ]\n",
      "Avg Train Accuracy =  0.9341100000000001\n",
      "LR Cross Validation Accuracy =  [0.8456 0.836  0.8378 0.8404 0.8388]\n",
      "Avg Validation Accuracy =  0.83972\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.98035 0.98145 0.9811  0.9785  0.97925]\n",
      "Avg Train Accuracy =  0.9801300000000002\n",
      "LR Cross Validation Accuracy =  [0.7958 0.7972 0.7884 0.7874 0.7808]\n",
      "Avg Validation Accuracy =  0.7899200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9085  0.9082  0.9079  0.9072  0.90545]\n",
      "Avg Train Accuracy =  0.9074500000000001\n",
      "LR Cross Validation Accuracy =  [0.8556 0.8474 0.8456 0.8456 0.854 ]\n",
      "Avg Validation Accuracy =  0.84964\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF * IDF, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.98965 0.9915  0.99145 0.99045 0.9902 ]\n",
      "Avg Train Accuracy =  0.9906499999999999\n",
      "LR Cross Validation Accuracy =  [0.8648 0.8318 0.8442 0.8542 0.8648]\n",
      "Avg Validation Accuracy =  0.85196\n"
     ]
    }
   ],
   "source": [
    "print(\"TF * IDF, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending Abbr. and All Lower Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extending sentences\n",
      "[=================== ] 99%\n",
      "Complete extending sentences\n"
     ]
    }
   ],
   "source": [
    "extended = extendSentences(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Unigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.87455 0.87815 0.8771  0.87505 0.87445]\n",
      "Avg Train Accuracy =  0.87586\n",
      "LR Cross Validation Accuracy =  [0.85   0.844  0.8422 0.845  0.8522]\n",
      "Avg Validation Accuracy =  0.8466799999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Unigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Bigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9297  0.93135 0.9309  0.92865 0.9294 ]\n",
      "Avg Train Accuracy =  0.93\n",
      "LR Cross Validation Accuracy =  [0.8438 0.8364 0.8382 0.8398 0.836 ]\n",
      "Avg Validation Accuracy =  0.83884\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Bigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Trigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9775  0.97865 0.9776  0.9758  0.97615]\n",
      "Avg Train Accuracy =  0.97714\n",
      "LR Cross Validation Accuracy =  [0.804  0.7966 0.7928 0.7926 0.7892]\n",
      "Avg Validation Accuracy =  0.79504\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Trigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 All-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Allgram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.906   0.90595 0.9065  0.906   0.9046 ]\n",
      "Avg Train Accuracy =  0.90581\n",
      "LR Cross Validation Accuracy =  [0.853  0.8476 0.847  0.8456 0.8534]\n",
      "Avg Validation Accuracy =  0.84932\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Allgram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Extended Abbr. Without Stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing punctuation and stopwords\n",
      "[=================== ] 99%\n",
      "Complete removing punctuation and stopwords\n"
     ]
    }
   ],
   "source": [
    "extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9114  0.91475 0.91325 0.91255 0.9107 ]\n",
      "Avg Train Accuracy =  0.91253\n",
      "LR Cross Validation Accuracy =  [0.8634 0.8574 0.8526 0.8584 0.8676]\n",
      "Avg Validation Accuracy =  0.8598800000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9824  0.9836  0.9823  0.9816  0.98215]\n",
      "Avg Train Accuracy =  0.98241\n",
      "LR Cross Validation Accuracy =  [0.7954 0.7806 0.7828 0.797  0.7932]\n",
      "Avg Validation Accuracy =  0.7898000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9996  0.99965 0.9995  0.9996  0.9995 ]\n",
      "Avg Train Accuracy =  0.9995700000000001\n",
      "LR Cross Validation Accuracy =  [0.683  0.6686 0.6474 0.6776 0.6642]\n",
      "Avg Validation Accuracy =  0.66816\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93305 0.935   0.93425 0.9334  0.9319 ]\n",
      "Avg Train Accuracy =  0.93352\n",
      "LR Cross Validation Accuracy =  [0.8582 0.8536 0.8448 0.8524 0.861 ]\n",
      "Avg Validation Accuracy =  0.8539999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Extended Abbr. Without Stopwords and Punctuations and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing\n",
      "[=================== ] 99%\n",
      "Complete lemmatizing\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.90855 0.91175 0.91045 0.90895 0.9082 ]\n",
      "Avg Train Accuracy =  0.90958\n",
      "LR Cross Validation Accuracy =  [0.8626 0.8558 0.85   0.856  0.8606]\n",
      "Avg Validation Accuracy =  0.857\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.97755 0.9785  0.977   0.97685 0.97875]\n",
      "Avg Train Accuracy =  0.97773\n",
      "LR Cross Validation Accuracy =  [0.7924 0.77   0.7716 0.7922 0.7932]\n",
      "Avg Validation Accuracy =  0.78388\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.999   0.99885 0.99895 0.99915 0.9988 ]\n",
      "Avg Train Accuracy =  0.9989500000000001\n",
      "LR Cross Validation Accuracy =  [0.663  0.6524 0.64   0.666  0.6526]\n",
      "Avg Validation Accuracy =  0.6548\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93105 0.9331  0.93175 0.9306  0.9293 ]\n",
      "Avg Train Accuracy =  0.93116\n",
      "LR Cross Validation Accuracy =  [0.855  0.85   0.8448 0.8508 0.8586]\n",
      "Avg Validation Accuracy =  0.8518399999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing duplicates\n",
      "[=================== ] 99%\n",
      "Complete removing duplicates\n"
     ]
    }
   ],
   "source": [
    "noDuplicates = removeDuplicates(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9131  0.91605 0.9146  0.9125  0.91075]\n",
      "Avg Train Accuracy =  0.9134\n",
      "LR Cross Validation Accuracy =  [0.8662 0.8652 0.8654 0.8668 0.8724]\n",
      "Avg Validation Accuracy =  0.8672000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9882  0.9881  0.98885 0.9876  0.9883 ]\n",
      "Avg Train Accuracy =  0.9882099999999999\n",
      "LR Cross Validation Accuracy =  [0.7654 0.7532 0.7556 0.761  0.7682]\n",
      "Avg Validation Accuracy =  0.76068\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.99935 0.9992  0.99925 0.99955 0.9994 ]\n",
      "Avg Train Accuracy =  0.99935\n",
      "LR Cross Validation Accuracy =  [0.637  0.6176 0.6068 0.628  0.6316]\n",
      "Avg Validation Accuracy =  0.6242\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9381  0.9396  0.93935 0.9377  0.93645]\n",
      "Avg Train Accuracy =  0.9382400000000001\n",
      "LR Cross Validation Accuracy =  [0.859  0.8566 0.8578 0.8598 0.8624]\n",
      "Avg Validation Accuracy =  0.8591200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Determine Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectLGParameters(dataSet,leastNgram, mostNgram, tfidf):\n",
    "    bestScore = 0\n",
    "    bestC = 0\n",
    "    output = list()\n",
    "    for i in range(100):\n",
    "        \n",
    "        #crossValidation(\"LR\", X,y_train, fold, c, train_score):\n",
    "        \n",
    "        train_set,valid_set=sklearn.model_selection.train_test_split(dataSet,train_size=0.8,test_size=0.2,shuffle=True)\n",
    "        \n",
    "        train_x=[comment[0] for comment in train_set]\n",
    "        train_y=[comment[1] for comment in train_set]\n",
    "        valid_x=[comment[0] for comment in valid_set]\n",
    "        valid_y=[comment[1] for comment in valid_set]\n",
    "\n",
    "        vectorizer = CountVectorizer(ngram_range=(leastNgram, mostNgram))\n",
    "        X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "        X = tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "        lg=LogisticRegression(C=1)\n",
    "        lg.fit(X,train_y)\n",
    "        X_valid=vectorizer.transform(valid_x)\n",
    "        valid_pred=lg.predict(X_valid)\n",
    "        score = sklearn.metrics.accuracy_score(valid_y, valid_pred)\n",
    "        if score > bestScore:\n",
    "            bestC = i\n",
    "        print(\"C = \", 1+i, \", Accuracy = \", score)\n",
    "        output.append([bestC, bestScore])\n",
    "    print(\"Best LR C = \", bestC, \"\\nAccuracy = \", bestScore)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extending sentences\n",
      "[=================== ] 99%\n",
      "Complete extending sentences\n",
      "Start removing punctuation and stopwords\n",
      "[=================== ] 99%\n",
      "Complete removing punctuation and stopwords\n",
      "Start lemmatizing\n",
      "[=================== ] 99%\n",
      "Complete lemmatizing\n",
      "Start removing duplicates\n",
      "[=================== ] 99%\n",
      "Complete removing duplicates\n"
     ]
    }
   ],
   "source": [
    "extended = extendSentences(train_data)\n",
    "extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])\n",
    "lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)\n",
    "noDuplicates = removeDuplicates(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  1 , Accuracy =  0.846\n",
      "C =  2 , Accuracy =  0.8706\n",
      "C =  3 , Accuracy =  0.8696\n",
      "C =  4 , Accuracy =  0.8794\n",
      "C =  5 , Accuracy =  0.8846\n",
      "C =  6 , Accuracy =  0.877\n",
      "C =  7 , Accuracy =  0.8858\n",
      "C =  8 , Accuracy =  0.8834\n",
      "C =  9 , Accuracy =  0.8828\n",
      "C =  10 , Accuracy =  0.8872\n",
      "C =  11 , Accuracy =  0.8798\n",
      "C =  12 , Accuracy =  0.8888\n",
      "C =  13 , Accuracy =  0.8858\n",
      "C =  14 , Accuracy =  0.8822\n",
      "C =  15 , Accuracy =  0.887\n",
      "C =  16 , Accuracy =  0.8838\n",
      "C =  17 , Accuracy =  0.8924\n",
      "C =  18 , Accuracy =  0.8844\n",
      "C =  19 , Accuracy =  0.8774\n",
      "C =  20 , Accuracy =  0.8896\n",
      "C =  21 , Accuracy =  0.878\n",
      "C =  22 , Accuracy =  0.8882\n",
      "C =  23 , Accuracy =  0.889\n",
      "C =  24 , Accuracy =  0.8838\n",
      "C =  25 , Accuracy =  0.8968\n",
      "C =  26 , Accuracy =  0.8842\n",
      "C =  27 , Accuracy =  0.8864\n",
      "C =  28 , Accuracy =  0.8848\n",
      "C =  29 , Accuracy =  0.882\n",
      "C =  30 , Accuracy =  0.8802\n",
      "C =  31 , Accuracy =  0.884\n",
      "C =  32 , Accuracy =  0.8944\n",
      "C =  33 , Accuracy =  0.8908\n",
      "C =  34 , Accuracy =  0.8846\n",
      "C =  35 , Accuracy =  0.8846\n",
      "C =  36 , Accuracy =  0.8904\n",
      "C =  37 , Accuracy =  0.8836\n",
      "C =  38 , Accuracy =  0.8846\n",
      "C =  39 , Accuracy =  0.8918\n",
      "C =  40 , Accuracy =  0.8894\n",
      "C =  41 , Accuracy =  0.8892\n",
      "C =  42 , Accuracy =  0.8842\n",
      "C =  43 , Accuracy =  0.8948\n",
      "C =  44 , Accuracy =  0.8928\n",
      "C =  45 , Accuracy =  0.8898\n",
      "C =  46 , Accuracy =  0.8866\n",
      "C =  47 , Accuracy =  0.8928\n",
      "C =  48 , Accuracy =  0.8874\n",
      "C =  49 , Accuracy =  0.887\n",
      "C =  50 , Accuracy =  0.8912\n",
      "C =  51 , Accuracy =  0.8962\n",
      "C =  52 , Accuracy =  0.8958\n",
      "C =  53 , Accuracy =  0.8876\n",
      "C =  54 , Accuracy =  0.8932\n",
      "C =  55 , Accuracy =  0.8898\n",
      "C =  56 , Accuracy =  0.8846\n",
      "C =  57 , Accuracy =  0.89\n",
      "C =  58 , Accuracy =  0.8864\n",
      "C =  59 , Accuracy =  0.8898\n",
      "C =  60 , Accuracy =  0.8906\n",
      "C =  61 , Accuracy =  0.887\n",
      "C =  62 , Accuracy =  0.885\n",
      "C =  63 , Accuracy =  0.896\n",
      "C =  64 , Accuracy =  0.8886\n",
      "C =  65 , Accuracy =  0.8872\n",
      "C =  66 , Accuracy =  0.8928\n",
      "C =  67 , Accuracy =  0.8922\n",
      "C =  68 , Accuracy =  0.886\n",
      "C =  69 , Accuracy =  0.893\n",
      "C =  70 , Accuracy =  0.8896\n",
      "C =  71 , Accuracy =  0.8904\n",
      "C =  72 , Accuracy =  0.884\n",
      "C =  73 , Accuracy =  0.8914\n",
      "C =  74 , Accuracy =  0.8934\n",
      "C =  75 , Accuracy =  0.8806\n",
      "C =  76 , Accuracy =  0.881\n",
      "C =  77 , Accuracy =  0.8932\n",
      "C =  78 , Accuracy =  0.8846\n",
      "C =  79 , Accuracy =  0.889\n",
      "C =  80 , Accuracy =  0.8896\n",
      "C =  81 , Accuracy =  0.8842\n",
      "C =  82 , Accuracy =  0.8858\n",
      "C =  83 , Accuracy =  0.8874\n",
      "C =  84 , Accuracy =  0.8912\n",
      "C =  85 , Accuracy =  0.8904\n",
      "C =  86 , Accuracy =  0.8954\n",
      "C =  87 , Accuracy =  0.8852\n",
      "C =  88 , Accuracy =  0.8884\n",
      "C =  89 , Accuracy =  0.8866\n",
      "C =  90 , Accuracy =  0.887\n",
      "C =  91 , Accuracy =  0.8948\n",
      "C =  92 , Accuracy =  0.8822\n",
      "C =  93 , Accuracy =  0.8876\n",
      "C =  94 , Accuracy =  0.8846\n",
      "C =  95 , Accuracy =  0.8844\n",
      "C =  96 , Accuracy =  0.888\n",
      "C =  97 , Accuracy =  0.8896\n",
      "C =  98 , Accuracy =  0.8788\n",
      "C =  99 , Accuracy =  0.8824\n",
      "C =  100 , Accuracy =  0.8872\n",
      "Best LR C =  99 \n",
      "Accuracy =  0\n"
     ]
    }
   ],
   "source": [
    "result = selectLGParameters(train_data,1, 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X,y_train,x_valid,y_valid, c):\n",
    "    lg=LogisticRegression(C=c)\n",
    "    lg.fit(X,y_train)\n",
    "    X_valid=vectorizer.transform(x_valid)\n",
    "    valid_pred=lg.predict(X_valid)\n",
    "    print (\"LR Accuracy: \", sklearn.metrics.accuracy_score(y_valid, valid_pred))\n",
    "    print (sklearn.metrics.confusion_matrix(y_valid, valid_pred, labels=[1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data=load_test()\n",
    "#extended = extendSentences(test_data)\n",
    "#extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])\n",
    "#lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)\n",
    "#noDuplicates = removeDuplicates(lemmatized)\n",
    "\n",
    "total_train_x=[comment[0] for comment in train_data]\n",
    "total_train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "test_x=[comment[0] for comment in test_data]\n",
    "test_id=[comment[1] for comment in test_data]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=vectorizer.fit_transform(total_train_x)\n",
    "train_X=tfidf_transformer.fit_transform(train_X)\n",
    "lg=LogisticRegression(C=100)\n",
    "lg.fit(train_X,total_train_y)\n",
    "X_valid=vectorizer.transform(test_x)\n",
    "X_valid=train_X=tfidf_transformer.fit_transform(X_valid)\n",
    "y_pred=lg.predict(X_valid)\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data={'Id':test_id,'Category':y_pred},columns=['Id','Category'])\n",
    "df.to_csv('D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\LR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
