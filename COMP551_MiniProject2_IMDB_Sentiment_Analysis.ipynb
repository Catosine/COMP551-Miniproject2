{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP551 Mini Project 2 - IMDB Sentiment Analysis  \n",
    "This is the codes of mini project2 - IMDB Sentiment Analysis.  \n",
    "\n",
    "## AUTHORS\n",
    "Pengnan Fan, ID#260768510  \n",
    "Qifei Zhao, ID#260719382  \n",
    "\n",
    "## TASKS\n",
    "1. **Bernoulli Naive Bayes** (w/o any external library).  \n",
    "2. **At least 2** out of 3 classifiers from the SciKit. i.e. suggestions: logistic regression, decision tree, or support vector machines  \n",
    "3. **At least 2** different features extraction pipelines for processing the data. \n",
    "4. A model validation. i.e. **K-fold cross validation**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Preparation  \n",
    "by Pengnan, Sherry, and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.1 - List of Packages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import numpy\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import contractions\n",
    "from itertools import groupby\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.2 - Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Please add your address here as string\n",
    "ADDRESS_TRAIN_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "ADDRESS_TEST_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan Fan\n",
    "# @param address of train set\n",
    "# @return a dict of list of dict\n",
    "def loadData(address):\n",
    "    print(\"Start loading negative set\")\n",
    "    neg = sklearn.datasets.load_files(address, categories={\"neg\"})\n",
    "    print(\"Complete loading negative set\")\n",
    "    print(\"Start loading positive set\")\n",
    "    pos = sklearn.datasets.load_files(address, categories={\"pos\"})\n",
    "    print(\"Complete loading postive set\")\n",
    "    \n",
    "    negSet = list()\n",
    "    count = 0\n",
    "    size = len(neg.data)\n",
    "    for x in neg.data:\n",
    "        negSet.append({\"comment\":x.decode('utf-8'), \"isPos\":0})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Preparing negative set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    posSet = list()\n",
    "    count = 0\n",
    "    size = len(pos.data)\n",
    "    for x in pos.data:\n",
    "        posSet.append({\"comment\":x.decode('utf-8'), \"isPos\":1})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Prepare positive set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    output = {'pos':posSet, 'neg':negSet, 'all':posSet+negSet}\n",
    "    print(\"\\nFinish preparing\")\n",
    "    return output\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Path\n",
    "train_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "test_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads train data to a list\n",
    "# @author\n",
    "# @param class_name: either \"pos\" for loading positive data and otherwise loading negative data\n",
    "# @return data: list of train data\n",
    "def load_train(class_name):\n",
    "    label=0\n",
    "    if class_name is \"pos\":\n",
    "        print(\"Start loading positive data\")\n",
    "        label=1\n",
    "    else:\n",
    "        print(\"Start loading negative data\")\n",
    "    data=[]\n",
    "    count = 0\n",
    "    lenW = 12500\n",
    "    for file in glob.glob(train_path+\"/\"+class_name+\"/*.txt\"):\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),label])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    if class_name is \"pos\":\n",
    "        print(\"\\nComplete loading positive data\")\n",
    "    else:\n",
    "        print(\"\\nComplete loading negative data\")\n",
    "    return data\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads test data to a list\n",
    "# @author\n",
    "# @return data: list of test data\n",
    "def load_test():\n",
    "    data=[]\n",
    "    print(\"Start loading test data\")\n",
    "    count = 0\n",
    "    lenW = 25000\n",
    "    for i in range(0,25000):\n",
    "        file=test_path+\"/\"+str(i)+\".txt\"\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),i])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete loading test data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set for prediction\n",
    "# @param wordExistance: dict {'pos' -> pos word existance, 'neg' -> neg word existance, 'all' -> all word existance}\n",
    "# @param wordSet: list of unique words\n",
    "# @param size: dict {'pos' -> pos size, 'neg' -> neg size, 'all' -> all size}\n",
    "# @return prediction\n",
    "def bernoulliNaiveBayes(dataSet, wordExistance, wordSet, size):\n",
    "    pPos = size['pos']/size['all']\n",
    "    pNeg = size['neg']/size['all']\n",
    "    prediction = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start Bernoulli naive Bayes classifying\")\n",
    "    for exp in dataSet:\n",
    "        #pPos_x = numpy.log([pPos])\n",
    "        #pNeg_x = numpy.log([pNeg])\n",
    "        \n",
    "        pred = numpy.log([pPos/nNeg])\n",
    "        \n",
    "        # Calculating P(Y|X)\n",
    "        for word in wordSet:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            \n",
    "            if word in exp:\n",
    "                pos = (wordExistance['pos'][word]+1)/(size['pos']+2)\n",
    "                neg = (wordExistance['neg'][word]+1)/(size['neg']+2)\n",
    "            else:\n",
    "                pos = 1 - ((wordExistance['pos'][word]+1)/(size['pos']+2))\n",
    "                neg = 1 - ((wordExistance['neg'][word]+1)/(size['neg']+2))\n",
    "            \n",
    "            #pPos_x += numpy.log([pos])\n",
    "            #pNeg_x += numpy.log([neg])\n",
    "            pred+=numpy.log([pos])\n",
    "        \n",
    "        # Logistic decision boundary\n",
    "        #log_ratio = pPos_x - pNeg_x\n",
    "        \n",
    "        if pred > 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print(\"\\nComplete Bernoulli naive Bayes classifying\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: train set with label\n",
    "# @param prediction: prediction of each example in the trainSet\n",
    "# @return result: list of results: {TP: true pos, TN: true neg, FP: false pos, FN: false neg}\n",
    "def evaluation(dataSet, prediction):\n",
    "    size = len(prediction)\n",
    "    count = 0\n",
    "    result = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    \n",
    "    print(\"Start evaluating classification\")\n",
    "    for i in range(size):\n",
    "        if prediction[i]==1:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['TP']+=1\n",
    "            else:\n",
    "                result['FP']+=1\n",
    "        else:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['FN']+=1\n",
    "            else:\n",
    "                result['TN']+=1\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete evaluating classification\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Logistic Regression, Support Vector Machine, and Decision Tree  \n",
    "by Sherry and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_data, 0.8, 0.2, True, False, 1, 3, 100)\n",
    "def LogReg(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN, c):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "    lg=LogisticRegression(C=c)\n",
    "    start_learn = time.time()\n",
    "    lg.fit(X,train_y)\n",
    "    end_learn = time.time()\n",
    "    #result = sklearn.model_selection.cross_validate(lg, X, y_train, cv=5, return_train_score=True)\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred = lg.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"LR Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score\n",
    "    \n",
    "    '''\n",
    "    print (\"LR Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"LR Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN, c):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    clf=SVC(kernel='linear', C=c)\n",
    "    start_learn = time.time()\n",
    "    clf.fit(X,train_y)\n",
    "    end_learn = time.time()\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred=clf.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"SVM Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score\n",
    "    #result = sklearn.model_selection.cross_validate(clf, X, y_train, cv=5, return_train_score=True)\n",
    "    '''\n",
    "    print (\"SVM Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"SVM Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecTree(train_data, train_ratio, test_ratio, random, tfidf, minN, maxN):\n",
    "    train_set,valid_set=sklearn.model_selection.train_test_split(train_data,train_size=train_ratio,test_size=test_ratio,shuffle=random)\n",
    "    train_x=[comment[0] for comment in train_set]\n",
    "    train_y=[comment[1] for comment in train_set]\n",
    "    valid_x=[comment[0] for comment in valid_set]\n",
    "    valid_y=[comment[1] for comment in valid_set]\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "     \n",
    "\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=2,max_leaf_nodes=300,min_samples_split=2)\n",
    "    start_learn = time.time()\n",
    "    dt.fit(X, train_y)\n",
    "    end_learn = time.time()\n",
    "    x_v=vectorizer.transform(valid_x)\n",
    "    start_pred = time.time()\n",
    "    y_pred = dt.predict(x_v)\n",
    "    end_pred = time.time()\n",
    "    score = sklearn.metrics.accuracy_score(valid_y, y_pred)\n",
    "    print (\"Decision Tree Accuracy: \", score)\n",
    "    print (\"Learning Running Time: \", end_learn - start_learn, \"seconds\")\n",
    "    print (\"Predicting Running Time: \", end_pred - start_pred, \"seconds\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Feature Extraction Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 - Word Frequency without Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @acknowledgement Yuxiang Ma, for this function is edited based on his in miniproject1\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: word frequency without stopwords\n",
    "def wordsFrequencyStopword(dataSet):\n",
    "    stopwordCount = dict()\n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of positive set\")\n",
    "    \n",
    "    for comment in dataSet['pos']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    posCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of positive set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of negative set\")\n",
    "    \n",
    "    for comment in dataSet['neg']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    negCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of negative set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of all set\")\n",
    "    \n",
    "    for comment in dataSet['all']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    allCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of all set\")\n",
    "    \n",
    "    stopwordCount = {'pos':posCountDict, 'neg':negCountDict, 'all':allCountDict}\n",
    "    \n",
    "    return stopwordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 - Number of Naive Existance of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of words of num of naive existances\n",
    "def numOfExistanceNaive(dataSet):\n",
    "    naiveCount = dict()\n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of positive set\")\n",
    "    for comment in dataSet['pos']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    posCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of positive set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of negative set\")\n",
    "    for comment in dataSet['neg']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    negCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of negative set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of all set\")\n",
    "    for comment in dataSet['all']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    allCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of all set\")\n",
    "    \n",
    "    naiveCount = {'pos':posCount, 'neg':negCount, 'all':allCount}\n",
    "    \n",
    "    return naiveCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.1 - Number of Existance of Words without Stopwords, Duplicates, and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords, duplicates, and with stemmer\n",
    "def advancedNumOfExistance(dataSet):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.2 - With Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords and with stemmer\n",
    "def advancedNumOfExistanceWithDuplicates(dataSet):\n",
    "    output = dict()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    eachFreqPos = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_pos = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqPos.append(temp_pos)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    eachFreqNeg = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "                \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_neg = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqNeg.append(temp_neg)\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    eachFreq = eachFreqPos+eachFreqNeg\n",
    "    \n",
    "    output = {'allFreq':countDict, 'individual':eachFreq}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.3 - Number of Existance of N-gram Words without Stopwords, Duplicates and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @param n: used for n-gram\n",
    "# @return countDict: list of num of existances of words without stopwords, duplicates and with stemmer\n",
    "def nGram(dataSet, n):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nCounting counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.4 - CountAllWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set used to learn\n",
    "# @return output: list of unique words shown in dataSet\n",
    "def allWords(dataSet):\n",
    "    output = list()\n",
    "    lenW = len(dataSet)\n",
    "    count = 0\n",
    "    print(\"Start counting all words\")\n",
    "    for comments in dataSet:\n",
    "        for word in comments:\n",
    "            if word not in output:\n",
    "                output.append(word)\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting all words\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4 - Data Standardization and Clean-up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet:comment data need to be proceed\n",
    "# @return proceedData\n",
    "def dataStandardization(dataSet):\n",
    "    proceedData = list()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    \n",
    "    print(\"Start proceeding data\")\n",
    "    for data in dataSet: \n",
    "        comment = contractions.fix(data['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        toAdd = list()\n",
    "        for x in processedComment:\n",
    "            if x not in stopwordsSet and x not in toAdd:\n",
    "                toAdd.append(x)\n",
    "                \n",
    "        proceedData.append(toAdd)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete proceeding data\")\n",
    "    return proceedData\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "# @param dataSet:set of data with comments\n",
    "# @return extendedSentence:set of comments without abbr. i.e. it's -> it is\n",
    "def extendSentences(dataSet):\n",
    "    extenedSentence = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start extending sentences\")\n",
    "    for data in dataSet:\n",
    "        extenedSentence.append([contractions.fix(data[0].lower()),data[1]])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete extending sentences\")\n",
    "    return extenedSentence\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "def removePunctuationAndStopwords(dataSet, additionalStopwords=[]):\n",
    "    clean = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    print(\"Start removing punctuation and stopwords\")\n",
    "    for data in dataSet:\n",
    "        comment = data[0].lower()\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        fixed = str()\n",
    "        for word in processedComment:\n",
    "            if word not in stopwordsSet and word not in additionalStopwords:\n",
    "                fixed = fixed + \" \" + word\n",
    "        clean.append([fixed,data[1]])\n",
    "    \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing punctuation and stopwords\")\n",
    "    return clean\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def lemmatize(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start lemmatizing\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        clean_text = str()\n",
    "        for word in words:\n",
    "            w = lemm.lemmatize(word)\n",
    "            clean_text += w + \" \"\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete lemmatizing\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def removeDuplicates(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start removing duplicates\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        noDup = list()\n",
    "        for word in words:\n",
    "            if word not in noDup:\n",
    "                noDup.append(word)\n",
    "                \n",
    "        clean_text = str()\n",
    "        for w in noDup:\n",
    "            clean_text = clean_text + \" \" + w\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing duplicates\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Qifei, Pengnan\n",
    "# @param text: a string of comment\n",
    "# @param custom_stopwords:list of additional stopwords\n",
    "# @return clean_txt: a string of clean-up comment\n",
    "def cleanUp(text, custom_stopwords=[]):\n",
    "    # Initilaise Lemmatizer object:\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Extend words\n",
    "    comment = contractions.fix(text.lower())\n",
    "    sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "    sent_clean = comment.translate(sent_map)\n",
    "    \n",
    "    # Load NLTK stopwords:\n",
    "    my_stopwords = stopwords.words('english') + custom_stopwords\n",
    "    \n",
    "    clean_text = ''\n",
    "    \n",
    "    words = word_tokenize(sent_clean) # word_tokenize() takes care of stripping too.\n",
    "    \n",
    "    for word in words:\n",
    "        w = lemm.lemmatize(w)\n",
    "        if w not in my_stopwords and len(w)>2:\n",
    "            clean_text += w + \" \"\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author\n",
    "# @param dataset: list of comments\n",
    "def cleanUp_data(dataset):\n",
    "    for i in range(0,len(dataset)):\n",
    "        comment=cleanUp(dataset[i][0])\n",
    "        dataset[i][0]=comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(mode, X,y_train, fold, c, train_score):\n",
    "    if mode is \"LR\":\n",
    "        lg=LogisticRegression(C=c)\n",
    "        lg.fit(X,y_train)\n",
    "        result = sklearn.model_selection.cross_validate(lg, X,y_train, cv=fold, return_train_score=train_score)\n",
    "        print (\"Logistic Regression Train Accuracy = \", result['train_score'])\n",
    "        print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "        print (\"Logistic Regression Cross Validation Accuracy = \", result['test_score'])\n",
    "        print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "        return result\n",
    "    \n",
    "    elif mode is \"SVM\":\n",
    "        clf=SVC(kernel='linear', C=c)\n",
    "        clf.fit(X,y_train)\n",
    "        result = sklearn.model_selection.cross_validate(clf, X, y_train, cv=fold, return_train_score=train_score)\n",
    "        print (\"SVM Train Accuracy = \", result['train_score'])\n",
    "        print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "        print (\"SVM Cross Validation Accuracy = \", result['test_score'])\n",
    "        print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))\n",
    "        return result\n",
    "    elif mode is \"DT\":\n",
    "        dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=2,max_leaf_nodes=300,min_samples_split=2)\n",
    "        dt.fit(X, y_train)\n",
    "        result = sklearn.model_selection.cross_validate(dt, X, y_train, cv=fold, return_train_score=train_score)\n",
    "    else:\n",
    "        print(\"INVALID INPUTS\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading positive data\n",
      "[=================== ] 99%\n",
      "Complete loading positive data\n",
      "Start loading negative data\n",
      "[=================== ] 99%\n",
      "Complete loading negative data\n",
      "Start loading test data\n",
      "[=================== ] 99%\n",
      "Complete loading test data\n"
     ]
    }
   ],
   "source": [
    "train_data=load_train(\"pos\")+load_train(\"neg\")\n",
    "test_data=load_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selecting Model Based on Accuracy and Runing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy:  0.8502\n",
      "Learning Running Time:  1.4989209175109863 seconds\n",
      "Predicting Running Time:  0.004904508590698242 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8502"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg(train_data, 0.8, 0.2, True, False, 1, 1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.8812\n",
      "Learning Running Time:  341.59919571876526 seconds\n",
      "Predicting Running Time:  84.45325541496277 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8812"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM(train_data, 0.8, 0.2, True, False, 1, 1, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.6848\n",
      "Learning Running Time:  10.170814037322998 seconds\n",
      "Predicting Running Time:  0.006982088088989258 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6848"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecTree(train_data, 0.8, 0.2, True, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectRegularizationParameter(mode, dataSet, minN, maxN, cv, tfidf):\n",
    "    train_x=[comment[0] for comment in dataSet]\n",
    "    train_y=[comment[1] for comment in dataSet]\n",
    "    \n",
    "    bestScore = 0\n",
    "    bestParameter = 0\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(minN, maxN))\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "    X=tfidf_transformer.fit_transform(X)\n",
    "    for i in range(100):\n",
    "        print(\"< Cross Validation\", i+1, \">\")\n",
    "        result = crossValidation(mode, X,train_y, cv, 1+i, True)\n",
    "        score = numpy.mean(result['test_score'])\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            bestParameter = i\n",
    "    print(\"Best Parameter: \", bestParameter,\", Best Score: \", bestScore)\n",
    "    return bestParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Cross Validation 1 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.87515 0.87805 0.87735 0.87565 0.87645]\n",
      "Avg Train Accuracy =  0.87653\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8506 0.8454 0.8444 0.844  0.8518]\n",
      "Avg Validation Accuracy =  0.84724\n",
      "< Cross Validation 2 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.89595 0.89745 0.8969  0.896   0.8949 ]\n",
      "Avg Train Accuracy =  0.89624\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8628 0.856  0.8554 0.855  0.8608]\n",
      "Avg Validation Accuracy =  0.858\n",
      "< Cross Validation 3 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9071  0.9084  0.9088  0.90645 0.9058 ]\n",
      "Avg Train Accuracy =  0.9073100000000001\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8676 0.8616 0.8556 0.8588 0.8662]\n",
      "Avg Validation Accuracy =  0.8619600000000001\n",
      "< Cross Validation 4 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.91465 0.9166  0.91635 0.91425 0.9131 ]\n",
      "Avg Train Accuracy =  0.9149899999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8692 0.8634 0.8572 0.8632 0.8684]\n",
      "Avg Validation Accuracy =  0.8642799999999999\n",
      "< Cross Validation 5 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9209  0.92335 0.92275 0.92015 0.91845]\n",
      "Avg Train Accuracy =  0.9211200000000002\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8696 0.8636 0.8584 0.8642 0.871 ]\n",
      "Avg Validation Accuracy =  0.8653600000000001\n",
      "< Cross Validation 6 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.92615 0.92875 0.928   0.92595 0.9247 ]\n",
      "Avg Train Accuracy =  0.9267099999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8698 0.8614 0.8586 0.8632 0.8726]\n",
      "Avg Validation Accuracy =  0.8651200000000001\n",
      "< Cross Validation 7 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.9303  0.9319  0.932   0.92965 0.92875]\n",
      "Avg Train Accuracy =  0.9305199999999999\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8706 0.8612 0.8586 0.8634 0.872 ]\n",
      "Avg Validation Accuracy =  0.86516\n",
      "< Cross Validation 8 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "parameter = selectRegularizationParameter(\"LR\", train_data, 1, 1, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy =  [0.87515 0.87805 0.87735 0.87565 0.87645]\n",
      "Avg Train Accuracy =  0.87653\n",
      "Logistic Regression Cross Validation Accuracy =  [0.8506 0.8454 0.8444 0.844  0.8518]\n",
      "Avg Validation Accuracy =  0.84724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.99115705, 0.99589705, 1.1870811 , 0.99713731, 0.96800184]),\n",
       " 'score_time': array([0.00500417, 0.00402164, 0.00398922, 0.00398922, 0.00399065]),\n",
       " 'test_score': array([0.8506, 0.8454, 0.8444, 0.844 , 0.8518]),\n",
       " 'train_score': array([0.87515, 0.87805, 0.87735, 0.87565, 0.87645])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No data processing Test, Unigram\")\n",
    "crossValidation(\"LR\", X,train_y, 5, 1.0, True)\n",
    "#LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValidation(\"LR\", X,train_y, 5, 1.0, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9329  0.9363  0.93415 0.9335  0.9337 ]\n",
      "Avg Train Accuracy =  0.9341100000000001\n",
      "LR Cross Validation Accuracy =  [0.8456 0.836  0.8378 0.8404 0.8388]\n",
      "Avg Validation Accuracy =  0.83972\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.98035 0.98145 0.9811  0.9785  0.97925]\n",
      "Avg Train Accuracy =  0.9801300000000002\n",
      "LR Cross Validation Accuracy =  [0.7958 0.7972 0.7884 0.7874 0.7808]\n",
      "Avg Validation Accuracy =  0.7899200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9085  0.9082  0.9079  0.9072  0.90545]\n",
      "Avg Train Accuracy =  0.9074500000000001\n",
      "LR Cross Validation Accuracy =  [0.8556 0.8474 0.8456 0.8456 0.854 ]\n",
      "Avg Validation Accuracy =  0.84964\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF * IDF, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.98965 0.9915  0.99145 0.99045 0.9902 ]\n",
      "Avg Train Accuracy =  0.9906499999999999\n",
      "LR Cross Validation Accuracy =  [0.8648 0.8318 0.8442 0.8542 0.8648]\n",
      "Avg Validation Accuracy =  0.85196\n"
     ]
    }
   ],
   "source": [
    "print(\"TF * IDF, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending Abbr. and All Lower Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extending sentences\n",
      "[=================== ] 99%\n",
      "Complete extending sentences\n"
     ]
    }
   ],
   "source": [
    "extended = extendSentences(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Unigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.87455 0.87815 0.8771  0.87505 0.87445]\n",
      "Avg Train Accuracy =  0.87586\n",
      "LR Cross Validation Accuracy =  [0.85   0.844  0.8422 0.845  0.8522]\n",
      "Avg Validation Accuracy =  0.8466799999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Unigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Bigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9297  0.93135 0.9309  0.92865 0.9294 ]\n",
      "Avg Train Accuracy =  0.93\n",
      "LR Cross Validation Accuracy =  [0.8438 0.8364 0.8382 0.8398 0.836 ]\n",
      "Avg Validation Accuracy =  0.83884\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Bigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Trigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9775  0.97865 0.9776  0.9758  0.97615]\n",
      "Avg Train Accuracy =  0.97714\n",
      "LR Cross Validation Accuracy =  [0.804  0.7966 0.7928 0.7926 0.7892]\n",
      "Avg Validation Accuracy =  0.79504\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Trigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 All-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Allgram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.906   0.90595 0.9065  0.906   0.9046 ]\n",
      "Avg Train Accuracy =  0.90581\n",
      "LR Cross Validation Accuracy =  [0.853  0.8476 0.847  0.8456 0.8534]\n",
      "Avg Validation Accuracy =  0.84932\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Allgram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Extended Abbr. Without Stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing punctuation and stopwords\n",
      "[=================== ] 99%\n",
      "Complete removing punctuation and stopwords\n"
     ]
    }
   ],
   "source": [
    "extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9114  0.91475 0.91325 0.91255 0.9107 ]\n",
      "Avg Train Accuracy =  0.91253\n",
      "LR Cross Validation Accuracy =  [0.8634 0.8574 0.8526 0.8584 0.8676]\n",
      "Avg Validation Accuracy =  0.8598800000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9824  0.9836  0.9823  0.9816  0.98215]\n",
      "Avg Train Accuracy =  0.98241\n",
      "LR Cross Validation Accuracy =  [0.7954 0.7806 0.7828 0.797  0.7932]\n",
      "Avg Validation Accuracy =  0.7898000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9996  0.99965 0.9995  0.9996  0.9995 ]\n",
      "Avg Train Accuracy =  0.9995700000000001\n",
      "LR Cross Validation Accuracy =  [0.683  0.6686 0.6474 0.6776 0.6642]\n",
      "Avg Validation Accuracy =  0.66816\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93305 0.935   0.93425 0.9334  0.9319 ]\n",
      "Avg Train Accuracy =  0.93352\n",
      "LR Cross Validation Accuracy =  [0.8582 0.8536 0.8448 0.8524 0.861 ]\n",
      "Avg Validation Accuracy =  0.8539999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Extended Abbr. Without Stopwords and Punctuations and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing\n",
      "[=================== ] 99%\n",
      "Complete lemmatizing\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.90855 0.91175 0.91045 0.90895 0.9082 ]\n",
      "Avg Train Accuracy =  0.90958\n",
      "LR Cross Validation Accuracy =  [0.8626 0.8558 0.85   0.856  0.8606]\n",
      "Avg Validation Accuracy =  0.857\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.97755 0.9785  0.977   0.97685 0.97875]\n",
      "Avg Train Accuracy =  0.97773\n",
      "LR Cross Validation Accuracy =  [0.7924 0.77   0.7716 0.7922 0.7932]\n",
      "Avg Validation Accuracy =  0.78388\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.999   0.99885 0.99895 0.99915 0.9988 ]\n",
      "Avg Train Accuracy =  0.9989500000000001\n",
      "LR Cross Validation Accuracy =  [0.663  0.6524 0.64   0.666  0.6526]\n",
      "Avg Validation Accuracy =  0.6548\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93105 0.9331  0.93175 0.9306  0.9293 ]\n",
      "Avg Train Accuracy =  0.93116\n",
      "LR Cross Validation Accuracy =  [0.855  0.85   0.8448 0.8508 0.8586]\n",
      "Avg Validation Accuracy =  0.8518399999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing duplicates\n",
      "[=================== ] 99%\n",
      "Complete removing duplicates\n"
     ]
    }
   ],
   "source": [
    "noDuplicates = removeDuplicates(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9131  0.91605 0.9146  0.9125  0.91075]\n",
      "Avg Train Accuracy =  0.9134\n",
      "LR Cross Validation Accuracy =  [0.8662 0.8652 0.8654 0.8668 0.8724]\n",
      "Avg Validation Accuracy =  0.8672000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9882  0.9881  0.98885 0.9876  0.9883 ]\n",
      "Avg Train Accuracy =  0.9882099999999999\n",
      "LR Cross Validation Accuracy =  [0.7654 0.7532 0.7556 0.761  0.7682]\n",
      "Avg Validation Accuracy =  0.76068\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.99935 0.9992  0.99925 0.99955 0.9994 ]\n",
      "Avg Train Accuracy =  0.99935\n",
      "LR Cross Validation Accuracy =  [0.637  0.6176 0.6068 0.628  0.6316]\n",
      "Avg Validation Accuracy =  0.6242\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9381  0.9396  0.93935 0.9377  0.93645]\n",
      "Avg Train Accuracy =  0.9382400000000001\n",
      "LR Cross Validation Accuracy =  [0.859  0.8566 0.8578 0.8598 0.8624]\n",
      "Avg Validation Accuracy =  0.8591200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Determine Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectLGParameters(dataSet,leastNgram, mostNgram, tfidf):\n",
    "    bestScore = 0\n",
    "    bestC = 0\n",
    "    output = list()\n",
    "    for i in range(100):\n",
    "        \n",
    "        #crossValidation(\"LR\", X,y_train, fold, c, train_score):\n",
    "        \n",
    "        train_set,valid_set=sklearn.model_selection.train_test_split(dataSet,train_size=0.8,test_size=0.2,shuffle=True)\n",
    "        \n",
    "        train_x=[comment[0] for comment in train_set]\n",
    "        train_y=[comment[1] for comment in train_set]\n",
    "        valid_x=[comment[0] for comment in valid_set]\n",
    "        valid_y=[comment[1] for comment in valid_set]\n",
    "\n",
    "        vectorizer = CountVectorizer(ngram_range=(leastNgram, mostNgram))\n",
    "        X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(use_idf=tfidf)\n",
    "        X = tfidf_transformer.fit_transform(X)\n",
    "    \n",
    "        lg=LogisticRegression(C=1)\n",
    "        lg.fit(X,train_y)\n",
    "        X_valid=vectorizer.transform(valid_x)\n",
    "        valid_pred=lg.predict(X_valid)\n",
    "        score = sklearn.metrics.accuracy_score(valid_y, valid_pred)\n",
    "        if score > bestScore:\n",
    "            bestC = i\n",
    "        print(\"C = \", 1+i, \", Accuracy = \", score)\n",
    "        output.append([bestC, bestScore])\n",
    "    print(\"Best LR C = \", bestC, \"\\nAccuracy = \", bestScore)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extending sentences\n",
      "[=================== ] 99%\n",
      "Complete extending sentences\n",
      "Start removing punctuation and stopwords\n",
      "[=================== ] 99%\n",
      "Complete removing punctuation and stopwords\n",
      "Start lemmatizing\n",
      "[=================== ] 99%\n",
      "Complete lemmatizing\n",
      "Start removing duplicates\n",
      "[=================== ] 99%\n",
      "Complete removing duplicates\n"
     ]
    }
   ],
   "source": [
    "extended = extendSentences(train_data)\n",
    "extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])\n",
    "lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)\n",
    "noDuplicates = removeDuplicates(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  1 , Accuracy =  0.846\n",
      "C =  2 , Accuracy =  0.8706\n",
      "C =  3 , Accuracy =  0.8696\n",
      "C =  4 , Accuracy =  0.8794\n",
      "C =  5 , Accuracy =  0.8846\n",
      "C =  6 , Accuracy =  0.877\n",
      "C =  7 , Accuracy =  0.8858\n",
      "C =  8 , Accuracy =  0.8834\n",
      "C =  9 , Accuracy =  0.8828\n",
      "C =  10 , Accuracy =  0.8872\n",
      "C =  11 , Accuracy =  0.8798\n",
      "C =  12 , Accuracy =  0.8888\n",
      "C =  13 , Accuracy =  0.8858\n",
      "C =  14 , Accuracy =  0.8822\n",
      "C =  15 , Accuracy =  0.887\n",
      "C =  16 , Accuracy =  0.8838\n",
      "C =  17 , Accuracy =  0.8924\n",
      "C =  18 , Accuracy =  0.8844\n",
      "C =  19 , Accuracy =  0.8774\n",
      "C =  20 , Accuracy =  0.8896\n",
      "C =  21 , Accuracy =  0.878\n",
      "C =  22 , Accuracy =  0.8882\n",
      "C =  23 , Accuracy =  0.889\n",
      "C =  24 , Accuracy =  0.8838\n",
      "C =  25 , Accuracy =  0.8968\n",
      "C =  26 , Accuracy =  0.8842\n",
      "C =  27 , Accuracy =  0.8864\n",
      "C =  28 , Accuracy =  0.8848\n",
      "C =  29 , Accuracy =  0.882\n",
      "C =  30 , Accuracy =  0.8802\n",
      "C =  31 , Accuracy =  0.884\n",
      "C =  32 , Accuracy =  0.8944\n",
      "C =  33 , Accuracy =  0.8908\n",
      "C =  34 , Accuracy =  0.8846\n",
      "C =  35 , Accuracy =  0.8846\n",
      "C =  36 , Accuracy =  0.8904\n",
      "C =  37 , Accuracy =  0.8836\n",
      "C =  38 , Accuracy =  0.8846\n",
      "C =  39 , Accuracy =  0.8918\n",
      "C =  40 , Accuracy =  0.8894\n",
      "C =  41 , Accuracy =  0.8892\n",
      "C =  42 , Accuracy =  0.8842\n",
      "C =  43 , Accuracy =  0.8948\n",
      "C =  44 , Accuracy =  0.8928\n",
      "C =  45 , Accuracy =  0.8898\n",
      "C =  46 , Accuracy =  0.8866\n",
      "C =  47 , Accuracy =  0.8928\n",
      "C =  48 , Accuracy =  0.8874\n",
      "C =  49 , Accuracy =  0.887\n",
      "C =  50 , Accuracy =  0.8912\n",
      "C =  51 , Accuracy =  0.8962\n",
      "C =  52 , Accuracy =  0.8958\n",
      "C =  53 , Accuracy =  0.8876\n",
      "C =  54 , Accuracy =  0.8932\n",
      "C =  55 , Accuracy =  0.8898\n",
      "C =  56 , Accuracy =  0.8846\n",
      "C =  57 , Accuracy =  0.89\n",
      "C =  58 , Accuracy =  0.8864\n",
      "C =  59 , Accuracy =  0.8898\n",
      "C =  60 , Accuracy =  0.8906\n",
      "C =  61 , Accuracy =  0.887\n",
      "C =  62 , Accuracy =  0.885\n",
      "C =  63 , Accuracy =  0.896\n",
      "C =  64 , Accuracy =  0.8886\n",
      "C =  65 , Accuracy =  0.8872\n",
      "C =  66 , Accuracy =  0.8928\n",
      "C =  67 , Accuracy =  0.8922\n",
      "C =  68 , Accuracy =  0.886\n",
      "C =  69 , Accuracy =  0.893\n",
      "C =  70 , Accuracy =  0.8896\n",
      "C =  71 , Accuracy =  0.8904\n",
      "C =  72 , Accuracy =  0.884\n",
      "C =  73 , Accuracy =  0.8914\n",
      "C =  74 , Accuracy =  0.8934\n",
      "C =  75 , Accuracy =  0.8806\n",
      "C =  76 , Accuracy =  0.881\n",
      "C =  77 , Accuracy =  0.8932\n",
      "C =  78 , Accuracy =  0.8846\n",
      "C =  79 , Accuracy =  0.889\n",
      "C =  80 , Accuracy =  0.8896\n",
      "C =  81 , Accuracy =  0.8842\n",
      "C =  82 , Accuracy =  0.8858\n",
      "C =  83 , Accuracy =  0.8874\n",
      "C =  84 , Accuracy =  0.8912\n",
      "C =  85 , Accuracy =  0.8904\n",
      "C =  86 , Accuracy =  0.8954\n",
      "C =  87 , Accuracy =  0.8852\n",
      "C =  88 , Accuracy =  0.8884\n",
      "C =  89 , Accuracy =  0.8866\n",
      "C =  90 , Accuracy =  0.887\n",
      "C =  91 , Accuracy =  0.8948\n",
      "C =  92 , Accuracy =  0.8822\n",
      "C =  93 , Accuracy =  0.8876\n",
      "C =  94 , Accuracy =  0.8846\n",
      "C =  95 , Accuracy =  0.8844\n",
      "C =  96 , Accuracy =  0.888\n",
      "C =  97 , Accuracy =  0.8896\n",
      "C =  98 , Accuracy =  0.8788\n",
      "C =  99 , Accuracy =  0.8824\n",
      "C =  100 , Accuracy =  0.8872\n",
      "Best LR C =  99 \n",
      "Accuracy =  0\n"
     ]
    }
   ],
   "source": [
    "result = selectLGParameters(train_data,1, 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X,y_train,x_valid,y_valid, c):\n",
    "    lg=LogisticRegression(C=c)\n",
    "    lg.fit(X,y_train)\n",
    "    X_valid=vectorizer.transform(x_valid)\n",
    "    valid_pred=lg.predict(X_valid)\n",
    "    print (\"LR Accuracy: \", sklearn.metrics.accuracy_score(y_valid, valid_pred))\n",
    "    print (sklearn.metrics.confusion_matrix(y_valid, valid_pred, labels=[1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data=load_test()\n",
    "#extended = extendSentences(test_data)\n",
    "#extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])\n",
    "#lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)\n",
    "#noDuplicates = removeDuplicates(lemmatized)\n",
    "\n",
    "total_train_x=[comment[0] for comment in train_data]\n",
    "total_train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "test_x=[comment[0] for comment in test_data]\n",
    "test_id=[comment[1] for comment in test_data]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=vectorizer.fit_transform(total_train_x)\n",
    "train_X=tfidf_transformer.fit_transform(train_X)\n",
    "lg=LogisticRegression(C=100)\n",
    "lg.fit(train_X,total_train_y)\n",
    "X_valid=vectorizer.transform(test_x)\n",
    "X_valid=train_X=tfidf_transformer.fit_transform(X_valid)\n",
    "y_pred=lg.predict(X_valid)\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data={'Id':test_id,'Category':y_pred},columns=['Id','Category'])\n",
    "df.to_csv('D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\LR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
