{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP551 Mini Project 2 - IMDB Sentiment Analysis  \n",
    "This is the codes of mini project2 - IMDB Sentiment Analysis.  \n",
    "\n",
    "## AUTHORS\n",
    "Pengnan Fan, ID#260768510  \n",
    "\n",
    "## TASKS\n",
    "1. **Bernoulli Naive Bayes** (w/o any external library).  \n",
    "2. **At least 2** out of 3 classifiers from the SciKit. i.e. suggestions: logistic regression, decision tree, or support vector machines  \n",
    "3. **At least 2** different features extraction pipelines for processing the data.  \n",
    "4. A model validation. i.e. **K-fold cross validation**  \n",
    "\n",
    "## UPDATES\n",
    "**<February 6, 2019>** Pengnan Fan create this notebook and implements functions readTrainData and readTestData  \n",
    "> * ~~**readTrainData(address:String):DataFrame**~~  \n",
    "> ~~This function takes a string **address** which indicates the address of your train data and will load comments and isPositive to a DataFrame.~~  \n",
    "> * ~~**readTestData(address:String):DataFrame**~~  \n",
    "> ~~This function takes a string **address** which indicates the address of your test data and will load comments and isPositive to a DataFrame. **Note: all isPositive is initialized as 0**~~  \n",
    "> * **Learning set** 25000 in total\n",
    "> * **Test set** 25000 in total  \n",
    "\n",
    "**<February 7, 2019>** Pengnan Fan implements wordsFrequencyNaive and wordsFrequencyStopword\n",
    "> * **wordsFrequencyNaive(dataSet:DataFrame)**  \n",
    "> This function takes a DataFrame **dataSet** and calculate the naive word frequency  \n",
    "> * **wordsFrequencyStopword(dataSet:DataFrame)**  \n",
    "> This function takes a DataFrame **dataSet** and calculate the word frequency without stopwords  \n",
    "\n",
    "**<February 9, 2019>** Pengnan Fan uses sklearn.datasets.load_files(address:str) to load all datas. It helps to increase the speed of loading. And the implementation of Bernoulli naive Bayes has been started.  \n",
    "> * **sklearn.datasets.load_files** - [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html)\n",
    "\n",
    "**<February 10, 2019>** Pengnan Fan implements numOfExistanceNaive and numOfExistanceStopword\n",
    "> * **numOfExistanceNaive(dataset:Bunch)**  \n",
    "> This function takes a Bunch **dataSet** and calculate the naive existance of words~~  \n",
    "> * **numOfExistanceStopword(dataset:Bunch)**  \n",
    "> This funciton takes a Bunch **dataSet** and calculate the existance of words without stopwords  \n",
    "\n",
    "**<February 11, 2019>** Pengnan Fan implements bernoulliNaiveBayes and evaluation. Also, the dataset is changed by using loadData  \n",
    "> * ~~**bernoulliNaiveBayes(dataSet:Bunch, totalWordFreq:set, negWordFreq:set, posWordFreq:set, numOfExamples:list)**~~  \n",
    "> ~~This function takes a Bunch **dataSet** as learning set and uses three sets of **totalWordFreq**, **negWordFreq**, **posWordFreq** and a list **numOfExamples** to calculates related probabilities. Note: It is not completely corrent. Fix later today.~~  \n",
    "> ~~* **evaluation(dataSet:Bunch, prediction:list)**  \n",
    "> This function takes a Bunch **dataSet** and a list **prediction** to generate a set containing true pos\\true neg\\false pos\\false neg\n",
    "> * **loadData(address:str)**~~  \n",
    "> This function takes a string **address** and generate a dict of 3 lists (pos, neg, all) of dict ('comment', 'isPos')  \n",
    "\n",
    "**<February 13, 2019>** Pengnan Fan fixs numOfExistanceStopword and bernoulliNaiveBayes\n",
    "> * **advancedNumOfExistance(dataset:dict of list of dict)**  \n",
    "> This funciton takes a dict of list of dict **dataSet** and calculate the existance of words without stopwords, puctuations, and duplicates.  \n",
    "> * **bernoulliNaiveBayes(dataSet:list of dict, wordExistance:dict of list, size:dict of list)**  \n",
    "> This function takes a list of dict **dataSet** as predicting set and uses a dict of list **wordExistance** and a dict of list **size** to calculates related probabilities.  \n",
    "> * **evaluation(dataSet:list of dict, prediction:list)**  \n",
    "> This function takes a list of dict **dataSet** and a list **prediction** to generate a set containing true pos\\true neg\\false pos\\false neg  \n",
    "> * **Outcome** TP =  11848, TN =  11482, FP =  1018, FN =  652  \n",
    ">> accuracy on train set =  93.32 %  \n",
    ">> accuracy on test set = 50%  \n",
    "> **Possible Reason** Overfitting caused by too many features.  \n",
    "\n",
    "**<February 18, 2019>** Pengnan finished crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import math\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import numpy\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import contractions\n",
    "from itertools import groupby\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.1 - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading negative set\n",
      "Complete loading negative set\n",
      "Start loading positive set\n",
      "Complete loading postive set\n",
      "Preparing negative set: [=================== ] 99%\n",
      "Prepare positive set: [=================== ] 99%\n",
      "Finish preparing\n"
     ]
    }
   ],
   "source": [
    "# Task 0 - Preprocessing Data\n",
    "\n",
    "# Please add your address here as string\n",
    "ADDRESS_TRAIN_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "# ADDRESS_TEST_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "\n",
    "\n",
    "# @author Pengnan Fan\n",
    "# @param address of train set\n",
    "# @return a dict of list of dict\n",
    "def loadData(address):\n",
    "    print(\"Start loading negative set\")\n",
    "    neg = sklearn.datasets.load_files(address, categories={\"neg\"})\n",
    "    print(\"Complete loading negative set\")\n",
    "    print(\"Start loading positive set\")\n",
    "    pos = sklearn.datasets.load_files(address, categories={\"pos\"})\n",
    "    print(\"Complete loading postive set\")\n",
    "    \n",
    "    negSet = list()\n",
    "    count = 0\n",
    "    size = len(neg.data)\n",
    "    for x in neg.data:\n",
    "        negSet.append({\"comment\":x.decode('utf-8'), \"isPos\":0})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Preparing negative set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    posSet = list()\n",
    "    count = 0\n",
    "    size = len(pos.data)\n",
    "    for x in pos.data:\n",
    "        posSet.append({\"comment\":x.decode('utf-8'), \"isPos\":1})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Prepare positive set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    output = {'pos':posSet, 'neg':negSet, 'all':posSet+negSet}\n",
    "    print(\"\\nFinish preparing\")\n",
    "    return output\n",
    "\n",
    "# Advanced loader\n",
    "# @return: Bunch\n",
    "\n",
    "trainSet = loadData(ADDRESS_TRAIN_PENGNAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0.2 - Naive Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start counting naive word frequency of positive set\n",
      "[=================== ] 99%\n",
      "Complete counting naive word frequency of positive set\n",
      "Start counting naive word frequency of negative set\n",
      "[=================== ] 99%\n",
      "Complete counting naive word frequency of negative set\n",
      "Start counting naive word frequency of all set\n",
      "[=================== ] 99%\n",
      "Complete counting naive word frequency of all set\n"
     ]
    }
   ],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: naive word frequency\n",
    "def wordsFrequencyNaive(dataSet):\n",
    "    naiveCount = dict()\n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of positive set\")\n",
    "    for comment in dataSet['pos']:\n",
    "        totalComments+=comment['comment'].lower().split(\" \")\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting naive word frequency of positive set\")\n",
    "    \n",
    "    posCount = Counter(x for x in totalComments)\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of negative set\")\n",
    "    for comment in dataSet['neg']:\n",
    "        totalComments+=comment['comment'].lower().split(\" \")\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting naive word frequency of negative set\")\n",
    "    \n",
    "    negCount = Counter(x for x in totalComments)\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of all set\")\n",
    "    for comment in dataSet['all']:\n",
    "        totalComments+=comment['comment'].lower().split(\" \")\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting naive word frequency of all set\")\n",
    "    \n",
    "    allCount = Counter(x for x in totalComments)\n",
    "    \n",
    "    naiveCount = {'pos':posCount, 'neg':negCount, 'all':allCount}\n",
    "    return naiveCount\n",
    "\n",
    "\n",
    "naiveFrequency = wordsFrequencyNaive(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(naiveFrequency.most_common(160))\\nprint(\"Size of naiveFrequency: \", len(naiveFrequency))\\nprint(\"Example: the = \", naiveFrequency[\\'the\\'])\\nprint(naiveFrequency_neg.most_common(160))\\nprint(\"Size of naiveFrequency_neg: \", len(naiveFrequency_neg))\\nprint(\"Example: the = \", naiveFrequency_neg[\\'the\\'])\\nprint(naiveFrequency_pos.most_common(160))\\nprint(\"Size of naiveFrequency_pos:\", len(naiveFrequency_pos))\\nprint(\"Example: the = \", naiveFrequency_pos[\\'the\\'])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of naive word frequency\n",
    "'''\n",
    "print(naiveFrequency.most_common(160))\n",
    "print(\"Size of naiveFrequency: \", len(naiveFrequency))\n",
    "print(\"Example: the = \", naiveFrequency['the'])\n",
    "print(naiveFrequency_neg.most_common(160))\n",
    "print(\"Size of naiveFrequency_neg: \", len(naiveFrequency_neg))\n",
    "print(\"Example: the = \", naiveFrequency_neg['the'])\n",
    "print(naiveFrequency_pos.most_common(160))\n",
    "print(\"Size of naiveFrequency_pos:\", len(naiveFrequency_pos))\n",
    "print(\"Example: the = \", naiveFrequency_pos['the'])\n",
    "'''\n",
    "# print(naiveFrequency['all'].most_common(160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.3 - Word Frequency without Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @acknowledgement Yuxiang Ma, for this function is edited based on his in miniproject1\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: word frequency without stopwords\n",
    "def wordsFrequencyStopword(dataSet):\n",
    "    stopwordCount = dict()\n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of positive set\")\n",
    "    \n",
    "    for comment in dataSet['pos']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    posCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of positive set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of negative set\")\n",
    "    \n",
    "    for comment in dataSet['neg']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    negCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of negative set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of all set\")\n",
    "    \n",
    "    for comment in dataSet['all']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    allCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of all set\")\n",
    "    \n",
    "    stopwordCount = {'pos':posCountDict, 'neg':negCountDict, 'all':allCountDict}\n",
    "    \n",
    "    return stopwordCount\n",
    "\n",
    "# stopwordFrequency = wordsFrequencyStopword(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(stopwordFrequency.most_common(160))\\nprint(\"Size of stopwordFrequency: \", len(stopwordFrequency))\\nprint(\"Example: br = \", stopwordFrequency[\\'br\\'])\\nprint(stopwordFrequency_neg.most_common(160))\\nprint(\"Size of stopwordFrequency_neg: \", len(stopwordFrequency_neg))\\nprint(\"Example: movie = \", stopwordFrequency_neg[\\'movie\\'])\\nprint(stopwordFrequency_pos.most_common(160))\\nprint(\"Size of stopwordFrequency_pos: \", len(stopwordFrequency_pos))\\nprint(\"Example: movie = \", stopwordFrequency_pos[\\'film\\'])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of word frequency without stopwords\n",
    "'''\n",
    "print(stopwordFrequency.most_common(160))\n",
    "print(\"Size of stopwordFrequency: \", len(stopwordFrequency))\n",
    "print(\"Example: br = \", stopwordFrequency['br'])\n",
    "print(stopwordFrequency_neg.most_common(160))\n",
    "print(\"Size of stopwordFrequency_neg: \", len(stopwordFrequency_neg))\n",
    "print(\"Example: movie = \", stopwordFrequency_neg['movie'])\n",
    "print(stopwordFrequency_pos.most_common(160))\n",
    "print(\"Size of stopwordFrequency_pos: \", len(stopwordFrequency_pos))\n",
    "print(\"Example: movie = \", stopwordFrequency_pos['film'])\n",
    "'''\n",
    "# print(stopwordFrequency['all'].most_common(160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.4 - Number of Naive Existance of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of words of num of naive existances\n",
    "def numOfExistanceNaive(dataSet):\n",
    "    naiveCount = dict()\n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of positive set\")\n",
    "    for comment in dataSet['pos']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    posCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of positive set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of negative set\")\n",
    "    for comment in dataSet['neg']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    negCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of negative set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of all set\")\n",
    "    for comment in dataSet['all']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    allCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of all set\")\n",
    "    \n",
    "    naiveCount = {'pos':posCount, 'neg':negCount, 'all':allCount}\n",
    "    \n",
    "    return naiveCount\n",
    "\n",
    "# naiveNumOfExistance = numOfExistanceNaive(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(naiveNumOfExistance.most_common(160))\\nprint(\"Size of naiveNumOfExistance: \", len(naiveNumOfExistance))\\nprint(\"Example: the = \", naiveNumOfExistance[\\'the\\'])\\nprint(naiveNumOfExistance_neg.most_common(160))\\nprint(\"Size of naiveNumOfExistance_neg: \", len(naiveNumOfExistance_neg))\\nprint(\"Example: the = \", naiveNumOfExistance_neg[\\'the\\'])\\nprint(naiveNumOfExistance_pos.most_common(160))\\nprint(\"Size of naiveNumOfExistance_pos: \", len(naiveNumOfExistance_pos))\\nprint(\"Example: the = \", naiveNumOfExistance_pos[\\'the\\'])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of number of naive existance of words\n",
    "'''\n",
    "print(naiveNumOfExistance.most_common(160))\n",
    "print(\"Size of naiveNumOfExistance: \", len(naiveNumOfExistance))\n",
    "print(\"Example: the = \", naiveNumOfExistance['the'])\n",
    "print(naiveNumOfExistance_neg.most_common(160))\n",
    "print(\"Size of naiveNumOfExistance_neg: \", len(naiveNumOfExistance_neg))\n",
    "print(\"Example: the = \", naiveNumOfExistance_neg['the'])\n",
    "print(naiveNumOfExistance_pos.most_common(160))\n",
    "print(\"Size of naiveNumOfExistance_pos: \", len(naiveNumOfExistance_pos))\n",
    "print(\"Example: the = \", naiveNumOfExistance_pos['the'])\n",
    "'''\n",
    "# print(naiveNumOfExistance['pos'].most_common(160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.5.1 - Number of Existance of Words without Stopwords and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start counting number of words without stopwords of existance of positive set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of positive set\n",
      "Start counting number of words without stopwords of existance of negative set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of negative set\n",
      "Start counting number of words without stopwords of existance of all set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of all set\n"
     ]
    }
   ],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords\n",
    "def advancedNumOfExistance(dataSet):\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict\n",
    "\n",
    "\n",
    "advancedNumOfExistance = advancedNumOfExistance(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 8102), ('one', 6817), ('film', 6647), ('like', 6168), ('would', 5325), ('even', 4905), ('good', 4762), ('bad', 4284), ('time', 4138), ('could', 4051), ('really', 4030), ('see', 3857), ('much', 3668), ('get', 3578), ('make', 3435), ('story', 3323), ('made', 3293), ('acting', 3240), ('people', 3133), ('plot', 3065), ('first', 3030), ('movies', 2909), ('well', 2900), ('way', 2892), ('think', 2771), ('watch', 2727), ('characters', 2639), ('better', 2637), ('seen', 2566), ('ever', 2508), ('know', 2495), ('never', 2468), ('say', 2393), ('little', 2333), ('films', 2333), ('cannot', 2318), ('nothing', 2299), ('thing', 2272), ('two', 2268), ('character', 2265), ('something', 2257), ('many', 2241), ('watching', 2152), ('go', 2138), ('great', 2060), ('scenes', 2027), ('worst', 2019), ('going', 1987), ('actually', 1968), ('actors', 1932), ('scene', 1905), ('another', 1872), ('back', 1868), ('still', 1862), ('look', 1819), ('life', 1720), ('best', 1703), ('got', 1699), ('least', 1694), ('real', 1691), ('us', 1675), ('pretty', 1672), ('every', 1671), ('director', 1656), ('script', 1649), ('minutes', 1644), ('though', 1639), ('enough', 1632), ('seems', 1614), ('give', 1579), ('anything', 1572), ('love', 1570), ('work', 1568), ('funny', 1561), ('find', 1556), ('lot', 1556), ('makes', 1553), ('show', 1552), ('old', 1548), ('around', 1548), ('fact', 1544), ('part', 1523), ('take', 1512), ('whole', 1502), ('thought', 1500), ('things', 1465), ('years', 1453), ('however', 1452), ('ive', 1443), ('point', 1432), ('might', 1410), ('long', 1394), ('cast', 1368), ('new', 1367), ('gets', 1354), ('without', 1351), ('interesting', 1339), ('big', 1339), ('quite', 1330), ('guy', 1329), ('far', 1321), ('money', 1320), ('almost', 1315), ('trying', 1307), ('making', 1302), ('right', 1297), ('must', 1297), ('saw', 1290), ('let', 1289), ('reason', 1288), ('original', 1264), ('done', 1256), ('sure', 1252), ('believe', 1251), ('looks', 1249), ('away', 1241), ('probably', 1239), ('kind', 1239), ('instead', 1236), ('horror', 1234), ('anyone', 1207), ('awful', 1206), ('waste', 1201), ('poor', 1200), ('someone', 1196), ('times', 1184), ('since', 1181), ('last', 1176), ('rather', 1165), ('2', 1162), ('may', 1152), ('boring', 1145), ('maybe', 1132), ('put', 1130), ('feel', 1127), ('looking', 1109), ('hard', 1108), ('found', 1096), ('goes', 1089), ('idea', 1089), ('bit', 1088), ('terrible', 1083), ('stupid', 1065), ('sense', 1060), ('comes', 1052), ('said', 1050), ('effects', 1048), ('seem', 1026), ('comedy', 1023), ('young', 1018), ('else', 1016), ('action', 1013), ('supposed', 1012), ('worse', 1011), ('main', 1011), ('watched', 1006), ('world', 1005), ('everything', 1002), ('completely', 998), ('tv', 996)]\n",
      "[('movie', 14993), ('one', 13791), ('film', 13558), ('like', 11440), ('would', 9587), ('good', 9408), ('even', 8346), ('time', 8265), ('see', 7953), ('really', 7597), ('story', 7399), ('much', 6981), ('could', 6819), ('well', 6782), ('get', 6717), ('first', 6298), ('great', 6228), ('made', 6136), ('people', 6056), ('make', 5962), ('way', 5819), ('bad', 5735), ('movies', 5478), ('think', 5396), ('watch', 5332), ('acting', 5239), ('seen', 5235), ('characters', 5196), ('films', 5050), ('many', 5049), ('never', 4982), ('plot', 4860), ('two', 4815), ('little', 4813), ('best', 4806), ('know', 4703), ('ever', 4673), ('character', 4577), ('better', 4514), ('still', 4382), ('love', 4345), ('say', 4304), ('life', 4246), ('go', 4080), ('cannot', 4010), ('something', 3938), ('scenes', 3827), ('back', 3796), ('watching', 3764), ('scene', 3697), ('thing', 3656), ('years', 3603), ('real', 3574), ('another', 3573), ('actors', 3544), ('makes', 3529), ('going', 3496), ('though', 3494), ('us', 3467), ('actually', 3422), ('look', 3378), ('nothing', 3370), ('find', 3370), ('work', 3324), ('every', 3287), ('show', 3266), ('lot', 3235), ('new', 3171), ('old', 3160), ('cast', 3126), ('got', 3107), ('director', 3106), ('part', 3098), ('funny', 3016), ('quite', 3015), ('things', 2968), ('give', 2954), ('take', 2945), ('fact', 2943), ('around', 2923), ('however', 2899), ('pretty', 2897), ('seems', 2891), ('enough', 2822), ('thought', 2804), ('without', 2793), ('ive', 2751), ('saw', 2733), ('long', 2732), ('big', 2719), ('young', 2691), ('times', 2688), ('least', 2686), ('right', 2675), ('may', 2671), ('must', 2665), ('always', 2659), ('world', 2646), ('almost', 2628), ('gets', 2617), ('whole', 2616), ('interesting', 2565), ('done', 2526), ('bit', 2525), ('point', 2516), ('anything', 2511), ('since', 2459), ('script', 2452), ('far', 2445), ('last', 2443), ('might', 2428), ('probably', 2424), ('feel', 2404), ('role', 2378), ('sure', 2335), ('away', 2335), ('minutes', 2324), ('original', 2306), ('making', 2278), ('kind', 2259), ('rather', 2257), ('anyone', 2251), ('let', 2241), ('worst', 2241), ('music', 2235), ('performance', 2224), ('found', 2222), ('comedy', 2218), ('yet', 2206), ('believe', 2188), ('especially', 2180), ('trying', 2162), ('comes', 2157), ('although', 2151), ('hard', 2148), ('course', 2145), ('goes', 2134), ('action', 2133), ('guy', 2132), ('put', 2115), ('played', 2085), ('worth', 2066), ('family', 2046), ('fun', 2042), ('day', 2002), ('looks', 1992), ('place', 1986), ('set', 1984), ('reason', 1978), ('looking', 1973), ('everything', 1971), ('tv', 1969), ('girl', 1967), ('woman', 1966), ('horror', 1960), ('watched', 1959), ('maybe', 1928), ('seem', 1917), ('sense', 1917), ('takes', 1914)]\n"
     ]
    }
   ],
   "source": [
    "# An example of number of existance of words without stopwords\n",
    "'''\n",
    "print(stopwordNumOfExistance.most_common(160))\n",
    "print(\"Size of stopwordNumOfExistance: \", len(stopwordNumOfExistance))\n",
    "print(\"Example: br = \", stopwordNumOfExistance['br'])\n",
    "print(naiveNumOfExistance_neg.most_common(160))\n",
    "print(\"Size of stopwordNumOfExistance_neg: \", len(stopwordNumOfExistance_neg))\n",
    "print(\"Example: movie = \", stopwordNumOfExistance_neg['movie'])\n",
    "print(naiveNumOfExistance_pos.most_common(160))\n",
    "print(\"Size of stopwordNumOfExistance_pos: \", len(stopwordNumOfExistance_pos))\n",
    "print(\"Example: movie = \", stopwordNumOfExistance_pos['film'])\n",
    "'''\n",
    "print(advancedNumOfExistance['neg'].most_common(160)) \n",
    "print(advancedNumOfExistance['all'].most_common(160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.5.2 - Without Eliminate Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start counting number of words without stopwords of existance of positive set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of positive set\n",
      "Start counting number of words without stopwords of existance of negative set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of negative set\n",
      "Start counting number of words without stopwords of existance of all set\n",
      "[=================== ] 99%\n",
      "Complete counting number of words without stopwords of existance of all set\n"
     ]
    }
   ],
   "source": [
    "def advancedNumOfExistanceWithDuplicates(dataSet):\n",
    "    output = dict()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    eachFreqPos = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_pos = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqPos.append(temp_pos)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    eachFreqNeg = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "                \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_neg = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqNeg.append(temp_neg)\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    eachFreq = eachFreqPos+eachFreqNeg\n",
    "    \n",
    "    output = {'allFreq':countDict, 'individual':eachFreq}\n",
    "    \n",
    "    return output\n",
    "\n",
    "wordsWithDuplicates = advancedNumOfExistanceWithDuplicates(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 41809), ('film', 37455), ('one', 25508), ('like', 19641), ('would', 15830), ('good', 14555), ('even', 12503), ('time', 11779), ('really', 11661), ('story', 11454), ('see', 11223), ('much', 9584), ('could', 9381), ('well', 9264), ('get', 9212), ('people', 8951), ('bad', 8912), ('great', 8894), ('first', 8857), ('made', 7990), ('movies', 7788), ('make', 7733), ('films', 7727), ('way', 7685), ('characters', 7291), ('think', 7229), ('watch', 6777), ('two', 6643), ('many', 6640), ('seen', 6530), ('character', 6514), ('never', 6425), ('little', 6387), ('acting', 6291), ('plot', 6275), ('best', 6263), ('love', 6214), ('show', 6083), ('know', 6038), ('life', 5988), ('ever', 5804), ('still', 5561), ('better', 5547), ('say', 5331), ('scene', 5169), ('scenes', 5063), ('go', 4991), ('something', 4892), ('cannot', 4811), ('back', 4760), ('us', 4724), ('watching', 4511), ('real', 4498), ('years', 4456), ('though', 4415), ('thing', 4389), ('actors', 4376), ('another', 4265), ('going', 4262), ('new', 4234), ('actually', 4211), ('nothing', 4202), ('makes', 4186), ('find', 4109), ('work', 4091), ('funny', 4072), ('look', 4053), ('old', 4031), ('every', 3960), ('lot', 3926), ('part', 3904), ('director', 3805), ('quite', 3714), ('got', 3703), ('cast', 3687), ('pretty', 3638), ('things', 3636), ('seems', 3604), ('young', 3596), ('around', 3531), ('fact', 3482), ('however', 3471), ('world', 3456), ('take', 3452), ('enough', 3377), ('give', 3371), ('may', 3349), ('ive', 3336), ('big', 3336), ('horror', 3311), ('original', 3297), ('thought', 3295), ('without', 3241), ('gets', 3212), ('always', 3206), ('series', 3198), ('right', 3173), ('long', 3164), ('saw', 3141), ('must', 3105)]\n",
      "Counter({'office': 2, 'doubt': 2, 'would': 2, 'conversation': 2, 'film': 2, 'going': 2, 'perhaps': 1, 'arrange': 1, 'meet': 1, 'sitting': 1, 'right': 1, 'facetoface': 1, 'bourne': 1, 'remains': 1, 'street': 1, 'tough': 1, 'elusive': 1, 'inhuman': 1, 'resilience': 1, 'leads': 1, 'little': 1, 'far': 1, 'fantasy': 1, 'macho': 1, 'point': 1, 'shards': 1, 'bond': 1, 'type': 1, 'gallows': 1, 'humour': 1, 'actionbr': 1, 'feeling': 1, 'something': 1, 'another': 1, 'level': 1, 'world': 1, 'live': 1, 'trilogy': 1, 'coveys': 1, 'well': 1, 'scene': 1, 'set': 1, 'waterloo': 1, 'guardian': 1, 'journalist': 1, 'great': 1, 'effect': 1, 'meeting': 1, 'worlds': 1, 'superfluous': 1, 'bodybr': 1, 'shaky': 1, 'annoy': 1, 'much': 1, 'enjoy': 1, 'hope': 1, 'somehow': 1, 'keep': 1, 'franchise': 1})\n"
     ]
    }
   ],
   "source": [
    "print(wordsWithDuplicates['allFreq']['all'].most_common(100))\n",
    "print(wordsWithDuplicates['individual'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.6 - Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start proceeding data\n",
      "[=================== ] 99%\n",
      "Complete proceeding data\n"
     ]
    }
   ],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet\n",
    "# @return proceedData\n",
    "def dataStandardization(dataSet):\n",
    "    proceedData = list()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    \n",
    "    print(\"Start proceeding data\")\n",
    "    for data in dataSet: \n",
    "        comment = contractions.fix(data['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        toAdd = list()\n",
    "        for x in processedComment:\n",
    "            if x not in stopwordsSet and x not in toAdd:\n",
    "                toAdd.append(x)\n",
    "                \n",
    "        proceedData.append(toAdd)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete proceeding data\")\n",
    "    return proceedData\n",
    "\n",
    "proceedData_All = dataStandardization(trainSet['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perhaps', 'arrange', 'meet', 'sitting', 'office', 'doubt', 'would', 'right', 'conversation', 'facetoface', 'bourne', 'remains', 'street', 'tough', 'elusive', 'inhuman', 'resilience', 'leads', 'film', 'little', 'far', 'fantasy', 'macho', 'point', 'shards', 'bond', 'type', 'gallows', 'humour', 'actionbr', 'feeling', 'something', 'going', 'another', 'level', 'world', 'live', 'trilogy', 'coveys', 'well', 'scene', 'set', 'waterloo', 'guardian', 'journalist', 'great', 'effect', 'meeting', 'worlds', 'superfluous', 'bodybr', 'shaky', 'annoy', 'much', 'enjoy', 'hope', 'somehow', 'keep', 'franchise']\n",
      "\n",
      "\"Perhaps we can arrange a meet. \" \"Where are you now? \" \"I'm sitting in my office. \" \"I doubt that. \" \"Why would you doubt that? \" \"If you were in your office right now we'd be having this conversation face-to-face. \"<br /><br />Bourne remains street tough, and elusive. Only his inhuman resilience leads the film a little too far into fantasy. Conversation is macho, to the point with only shards of Bond type gallows humour. Its all about the action.<br /><br />The feeling that there is something going on at another level to the world we live in is what the trilogy coveys so well. A scene set in Waterloo with a Guardian journalist does this to great effect. There is no meeting of worlds - you are in it or just a superfluous body.<br /><br />If the shaky cam doesn't annoy you too much, enjoy this film and hope they somehow keep the franchise going.\n"
     ]
    }
   ],
   "source": [
    "print(proceedData_All[0])\n",
    "print()\n",
    "print(trainSet['all'][0]['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.7.1 - CountAllWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start counting all words\n",
      "[=================== ] 99%\n",
      "Complete counting all words\n"
     ]
    }
   ],
   "source": [
    "def allWords(dataSet):\n",
    "    output = list()\n",
    "    lenW = len(dataSet)\n",
    "    count = 0\n",
    "    print(\"Start counting all words\")\n",
    "    for comments in dataSet:\n",
    "        for word in comments:\n",
    "            if word not in output:\n",
    "                output.append(word)\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting all words\")\n",
    "    return output\n",
    "\n",
    "allWord = allWords(proceedData_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perhaps', 'arrange', 'meet', 'sitting', 'office', 'doubt', 'would', 'right', 'conversation', 'facetoface', 'bourne', 'remains', 'street', 'tough', 'elusive', 'inhuman', 'resilience', 'leads', 'film', 'little', 'far', 'fantasy', 'macho', 'point', 'shards', 'bond', 'type', 'gallows', 'humour', 'actionbr', 'feeling', 'something', 'going', 'another', 'level', 'world', 'live', 'trilogy', 'coveys', 'well', 'scene', 'set', 'waterloo', 'guardian', 'journalist', 'great', 'effect', 'meeting', 'worlds', 'superfluous', 'bodybr', 'shaky', 'annoy', 'much', 'enjoy', 'hope', 'somehow', 'keep', 'franchise', 'movie', 'horrible', 'bad', 'good', 'kind', 'waybr', 'storyline', 'rehashed', 'many', 'films', 'even', 'bother', 'describing', 'swordsorcery', 'picture', 'kid', 'hoping', 'realize', 'important', 'nomadic', 'adventurer', 'evil', 'aidesorcerer', 'princess', 'hairy', 'creatureyou', 'get', 'pointbr', 'first', 'time', 'caught', 'harsh', 'winter', 'know', 'decided', 'continue', 'watching', 'extra', 'five', 'minutes', 'turning']\n"
     ]
    }
   ],
   "source": [
    "print(allWord[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.7.2 - Advanced CountAllWords (Select Words by IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start selecting words by information gain\n",
      "[                    ] 0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log2\n",
      "  app.launch_new_instance()\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%\n",
      "Complete selecting words by information gain\n"
     ]
    }
   ],
   "source": [
    "# @author Pengnan Fan\n",
    "# This function reduce features by information gain\n",
    "def selectWords(uniqueWordSet, wordExistance, sizeOfSet):\n",
    "    output = list()\n",
    "    lenW = len(uniqueWordSet)\n",
    "    count = 0\n",
    "    h_D = -(sizeOfSet['pos']/sizeOfSet['all'])*numpy.log2(sizeOfSet['pos']/sizeOfSet['all']) - (sizeOfSet['neg']/sizeOfSet['all'])*numpy.log2(sizeOfSet['neg']/sizeOfSet['all'])\n",
    "    print(\"Start selecting words by information gain\")\n",
    "    for word in uniqueWordSet:\n",
    "        size_with_word = wordExistance['pos'][word] + wordExistance['neg'][word]\n",
    "        rate_with_word = size_with_word/sizeOfSet['all']\n",
    "        \n",
    "        size_without_word = sizeOfSet['all'] - size_with_word\n",
    "        rate_without_word = 1 - rate_with_word\n",
    "        \n",
    "        h_C_with_word = rate_with_word*(-(wordExistance['pos'][word]/size_with_word)*numpy.log2(wordExistance['pos'][word]/size_with_word) - (wordExistance['neg'][word]/size_with_word)*numpy.log2(wordExistance['neg'][word]/size_with_word))\n",
    "        h_C_without_word = rate_without_word*(-((sizeOfSet['pos']-wordExistance['pos'][word])/size_without_word)*numpy.log2((sizeOfSet['pos']-wordExistance['pos'][word])/size_without_word) - ((sizeOfSet['neg']-wordExistance['neg'][word])/size_without_word)*numpy.log2((sizeOfSet['neg']-wordExistance['neg'][word])/size_without_word))\n",
    "        h_C = h_C_with_word + h_C_without_word\n",
    "        if h_D - h_C > 1:\n",
    "            output.append(word)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete selecting words by information gain\")\n",
    "    return output\n",
    "\n",
    "size = dict()\n",
    "size = {'pos':len(trainSet['pos']), 'neg':len(trainSet['neg']), 'all':len(trainSet['all'])}\n",
    "selectedWordSet = selectWords(allWord, advancedNumOfExistance, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   120777\n"
     ]
    }
   ],
   "source": [
    "print(len(selectedWordSet), \" \", len(allWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set for prediction\n",
    "# @param wordExistance: dict {'pos' -> pos word existance, 'neg' -> neg word existance, 'all' -> all word existance}\n",
    "# @param size: dict {'pos' -> pos size, 'neg' -> neg size, 'all' -> all size}\n",
    "# @return prediction\n",
    "def bernoulliNaiveBayes(dataSet, wordExistance, wordSet, size):\n",
    "    pPos = size['pos']/size['all']\n",
    "    pNeg = size['neg']/size['all']\n",
    "    prediction = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start Bernoulli naive Bayes classifying\")\n",
    "    for exp in dataSet:\n",
    "        pPos_x = numpy.log([pPos])\n",
    "        pNeg_x = numpy.log([pNeg])\n",
    "        \n",
    "        for word in wordSet:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            \n",
    "            if word in exp:\n",
    "                pos = (wordExistance['pos'][word]+1)/(size['pos']+2)\n",
    "                neg = (wordExistance['neg'][word]+1)/(size['neg']+2)\n",
    "            else:\n",
    "                pos = 1 - ((wordExistance['pos'][word]+1)/(size['pos']+2))\n",
    "                neg = 1 - ((wordExistance['neg'][word]+1)/(size['neg']+2))\n",
    "            \n",
    "            pPos_x += numpy.log([pos])\n",
    "            pNeg_x += numpy.log([neg])\n",
    "            \n",
    "        log_ratio = pPos_x - pNeg_x\n",
    "        \n",
    "        if log_ratio > 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print(\"\\nComplete Bernoulli naive Bayes classifying\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "#pred = bernoulliNaiveBayes(proceedData_All, advancedNumOfExistance, allWord, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.2 - Bernoulli Naive Bayes with TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulliNaiveBayesWithTFIDF(dataSet, wordExistance, wordSet, size, commentExistances):\n",
    "    pPos = size['pos']/size['all']\n",
    "    pNeg = size['neg']/size['all']\n",
    "    prediction = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start Bernoulli naive Bayes classifying\")\n",
    "    for i in range(lenW):\n",
    "        pPos_x = numpy.log([pPos])\n",
    "        pNeg_x = numpy.log([pNeg])\n",
    "        \n",
    "        for word in wordSet:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            \n",
    "            numOfWords = sum(commentExistances[i].values())\n",
    "            tf = (commentExistances[i][word])/numOfWords\n",
    "            idf = numpy.log([size['all']/wordExistance['all'][word]])\n",
    "            \n",
    "            if word in dataSet[i]:\n",
    "                pos = ((wordExistance['pos'][word]+1)/(size['pos']+2))*tf*idf\n",
    "                neg = ((wordExistance['neg'][word]+1)/(size['neg']+2))*tf*idf\n",
    "            else:\n",
    "                pos = (1 - ((wordExistance['pos'][word]+1)/(size['pos']+2)))*tf*idf\n",
    "                neg = (1 - ((wordExistance['neg'][word]+1)/(size['neg']+2)))*tf*idf\n",
    "            \n",
    "            pPos_x += numpy.log(list(pos))\n",
    "            pNeg_x += numpy.log(list(neg))\n",
    "            \n",
    "        log_ratio = pPos_x - pNeg_x\n",
    "        \n",
    "        if log_ratio > 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print(\"\\nComplete Bernoulli naive Bayes classifying\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "advancedPred = bernoulliNaiveBayesWithTFIDF(proceedData_All, advancedNumOfExistance, allWord, size, wordsWithDuplicates['individual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "65\n",
      "Counter({'office': 2, 'doubt': 2, 'would': 2, 'conversation': 2, 'film': 2, 'going': 2, 'perhaps': 1, 'arrange': 1, 'meet': 1, 'sitting': 1, 'right': 1, 'facetoface': 1, 'bourne': 1, 'remains': 1, 'street': 1, 'tough': 1, 'elusive': 1, 'inhuman': 1, 'resilience': 1, 'leads': 1, 'little': 1, 'far': 1, 'fantasy': 1, 'macho': 1, 'point': 1, 'shards': 1, 'bond': 1, 'type': 1, 'gallows': 1, 'humour': 1, 'actionbr': 1, 'feeling': 1, 'something': 1, 'another': 1, 'level': 1, 'world': 1, 'live': 1, 'trilogy': 1, 'coveys': 1, 'well': 1, 'scene': 1, 'set': 1, 'waterloo': 1, 'guardian': 1, 'journalist': 1, 'great': 1, 'effect': 1, 'meeting': 1, 'worlds': 1, 'superfluous': 1, 'bodybr': 1, 'shaky': 1, 'annoy': 1, 'much': 1, 'enjoy': 1, 'hope': 1, 'somehow': 1, 'keep': 1, 'franchise': 1})\n"
     ]
    }
   ],
   "source": [
    "print(len(proceedData_All[0]))\n",
    "print(sum(wordsWithDuplicates['individual'][0].values()))\n",
    "print(wordsWithDuplicates['individual'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating classification\n",
      "[=================== ] 99%\n",
      "Complete evaluating classification\n",
      "TP =  11120 \n",
      "TN =  11982 \n",
      "FP =  518 \n",
      "FN =  1380\n"
     ]
    }
   ],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: train set with label\n",
    "# @param prediction: prediction of each example in the trainSet\n",
    "# @return result: list of results: {TP: true pos, TN: true neg, FP: false pos, FN: false neg}\n",
    "def evaluation(dataSet, prediction):\n",
    "    size = len(prediction)\n",
    "    count = 0\n",
    "    result = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    \n",
    "    print(\"Start evaluating classification\")\n",
    "    for i in range(size):\n",
    "        if prediction[i]==1:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['TP']+=1\n",
    "            else:\n",
    "                result['FP']+=1\n",
    "        else:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['FN']+=1\n",
    "            else:\n",
    "                result['TN']+=1\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete evaluating classification\")\n",
    "    return result\n",
    "\n",
    "eva = evaluation(trainSet['all'], pred)\n",
    "print(\"TP = \", eva['TP'], \"\\nTN = \", eva['TN'], \"\\nFP = \", eva['FP'], \"\\nFN = \", eva['FN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  92.408 %\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy = \", (eva['TP']+eva['TN'])/250, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading test set\n",
      "Complete loading test set\n",
      "Preparing test set: [=================== ] 99%\n",
      "\n",
      "Finish preparing test set\n"
     ]
    }
   ],
   "source": [
    "ADDRESS_TEST_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "def loadTestData():\n",
    "    print(\"Start loading test set\")\n",
    "    test = sklearn.datasets.load_files(\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\", categories={\"test\"}, shuffle=False)\n",
    "    print(\"Complete loading test set\")\n",
    "    \n",
    "    testSet = list()\n",
    "    count = 0\n",
    "    size = len(test.data)\n",
    "    for x in test.data:\n",
    "        testSet.append({\"comment\":x.decode('utf-8'), \"isPos\":0})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Preparing test set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print()\n",
    "    print(\"\\nFinish preparing test set\")\n",
    "    return testSet\n",
    "\n",
    "testSet = loadTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start proceeding data\n",
      "[=================== ] 99%\n",
      "Complete proceeding data\n"
     ]
    }
   ],
   "source": [
    "editedData = dataStandardization(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120777\n"
     ]
    }
   ],
   "source": [
    "print(len(allWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Bernoulli naive Bayes classifying\n",
      "[=================== ] 99%\n",
      "Complete Bernoulli naive Bayes classifying\n"
     ]
    }
   ],
   "source": [
    "result = bernoulliNaiveBayes(editedData, advancedNumOfExistance, allWord, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "for i in range(25000):\n",
    "    index.append(i)\n",
    "print(index[0:100])\n",
    "outcome = {\"Id\":index, \"Category\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "print(outcome['Id'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outcome, columns={'Category', 'Id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Category\n",
      "0          0         0\n",
      "1          1         0\n",
      "2          2         1\n",
      "3          3         0\n",
      "4          4         0\n",
      "5          5         1\n",
      "6          6         1\n",
      "7          7         1\n",
      "8          8         0\n",
      "9          9         0\n",
      "10        10         0\n",
      "11        11         0\n",
      "12        12         0\n",
      "13        13         1\n",
      "14        14         1\n",
      "15        15         1\n",
      "16        16         1\n",
      "17        17         1\n",
      "18        18         0\n",
      "19        19         1\n",
      "20        20         0\n",
      "21        21         0\n",
      "22        22         0\n",
      "23        23         1\n",
      "24        24         1\n",
      "25        25         0\n",
      "26        26         0\n",
      "27        27         0\n",
      "28        28         0\n",
      "29        29         0\n",
      "...      ...       ...\n",
      "24970  24970         1\n",
      "24971  24971         0\n",
      "24972  24972         0\n",
      "24973  24973         1\n",
      "24974  24974         0\n",
      "24975  24975         1\n",
      "24976  24976         0\n",
      "24977  24977         1\n",
      "24978  24978         0\n",
      "24979  24979         1\n",
      "24980  24980         1\n",
      "24981  24981         0\n",
      "24982  24982         0\n",
      "24983  24983         1\n",
      "24984  24984         0\n",
      "24985  24985         1\n",
      "24986  24986         0\n",
      "24987  24987         1\n",
      "24988  24988         1\n",
      "24989  24989         0\n",
      "24990  24990         1\n",
      "24991  24991         0\n",
      "24992  24992         0\n",
      "24993  24993         0\n",
      "24994  24994         0\n",
      "24995  24995         1\n",
      "24996  24996         0\n",
      "24997  24997         0\n",
      "24998  24998         1\n",
      "24999  24999         1\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = df.to_csv ('D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\result.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet of comments\n",
    "def crossValidation(dataSet):\n",
    "    subSet_pos = list()\n",
    "    subSet_neg = list()\n",
    "    a = 0\n",
    "    b = 0\n",
    "    \n",
    "    # It separates the data set into 5 sub sets by pos/neg\n",
    "    # You may modify it based on your data structure\n",
    "    for x in range(5):\n",
    "        a = b\n",
    "        b += 2500\n",
    "        subSet_pos.append(dataSet['pos'][a:b])\n",
    "        subSet_neg.append(dataSet['neg'][a:b])\n",
    "    \n",
    "    # It loops 5 times for cross validations\n",
    "    for i in range(5):\n",
    "        \n",
    "        print(\"<Cross Validation\", i, \">\")\n",
    "        posSet = list()\n",
    "        negSet = list()\n",
    "        valSet = list()\n",
    "        cvSet = {}\n",
    "        \n",
    "        # It chooses 1 out of 5 subsets as validation set, and learn on the rest\n",
    "        for j in range(5):\n",
    "            if j != i:\n",
    "                # Preparing train set\n",
    "                posSet += subSet_pos[j]\n",
    "                negSet += subSet_neg[j]\n",
    "            else:\n",
    "                # Preparing validation set\n",
    "                valSet = subSet_pos[j] + subSet_neg[j]\n",
    "                \n",
    "        # -------------------------------------------------------------------- #\n",
    "        # Add your own proceeding functions and learning functions starts here\n",
    "        \n",
    "        prediction = someClassifier(SomeInputs)\n",
    "        \n",
    "        # Add your own proceeding functions and learning functions ends here\n",
    "        # ------------------------------------------------------------------ #\n",
    "        \n",
    "        # You may use the evaluation function I wrote here. Please check the input sturcture before use\n",
    "        result = evaluation(answer, prediction)\n",
    "        print(\"True Positive = \", result['TP'], \"\\nTrue Negative = \", result['TN'], \"\\nFalse Positive = \", result['FP'], \"\\nFalse Negative = \", result ['FN'])\n",
    "        print(\"Accuracy on validation set \", i, \" = \", (result['TP']+result['TF'])/len(valSet))\n",
    "        \n",
    "    print(\"\\nComplete cross validation\")\n",
    "\n",
    "crossValidation(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
