{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP551 Mini Project 2 - IMDB Sentiment Analysis  \n",
    "This is the codes of mini project2 - IMDB Sentiment Analysis.  \n",
    "\n",
    "## AUTHORS\n",
    "Pengnan Fan, ID#260768510  \n",
    "Qifei Zhao, ID#260719382  \n",
    "\n",
    "## TASKS\n",
    "1. **Bernoulli Naive Bayes** (w/o any external library).  \n",
    "2. **At least 2** out of 3 classifiers from the SciKit. i.e. suggestions: logistic regression, decision tree, or support vector machines  \n",
    "3. **At least 2** different features extraction pipelines for processing the data.  \n",
    "4. A model validation. i.e. **K-fold cross validation**  \n",
    "\n",
    "## UPDATES\n",
    "**<February 6, 2019>** Pengnan Fan create this notebook and implements functions readTrainData and readTestData  \n",
    "> * ~~**readTrainData(address:String):DataFrame**~~  \n",
    "> ~~This function takes a string **address** which indicates the address of your train data and will load comments and isPositive to a DataFrame.~~  \n",
    "> * ~~**readTestData(address:String):DataFrame**~~  \n",
    "> ~~This function takes a string **address** which indicates the address of your test data and will load comments and isPositive to a DataFrame. **Note: all isPositive is initialized as 0**~~  \n",
    "> * **Learning set** 25000 in total\n",
    "> * **Test set** 25000 in total  \n",
    "\n",
    "**<February 7, 2019>** Pengnan Fan implements wordsFrequencyNaive and wordsFrequencyStopword\n",
    "> * **wordsFrequencyNaive(dataSet:DataFrame)**  \n",
    "> This function takes a DataFrame **dataSet** and calculate the naive word frequency  \n",
    "> * **wordsFrequencyStopword(dataSet:DataFrame)**  \n",
    "> This function takes a DataFrame **dataSet** and calculate the word frequency without stopwords  \n",
    "\n",
    "**<February 9, 2019>** Pengnan Fan uses sklearn.datasets.load_files(address:str) to load all datas. It helps to increase the speed of loading. And the implementation of Bernoulli naive Bayes has been started.  \n",
    "> * **sklearn.datasets.load_files** - [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html)\n",
    "\n",
    "**<February 10, 2019>** Pengnan Fan implements numOfExistanceNaive and numOfExistanceStopword\n",
    "> * **numOfExistanceNaive(dataset:Bunch)**  \n",
    "> This function takes a Bunch **dataSet** and calculate the naive existance of words~~  \n",
    "> * **numOfExistanceStopword(dataset:Bunch)**  \n",
    "> This funciton takes a Bunch **dataSet** and calculate the existance of words without stopwords  \n",
    "\n",
    "**<February 11, 2019>** Pengnan Fan implements bernoulliNaiveBayes and evaluation. Also, the dataset is changed by using loadData  \n",
    "> * ~~**bernoulliNaiveBayes(dataSet:Bunch, totalWordFreq:set, negWordFreq:set, posWordFreq:set, numOfExamples:list)**~~  \n",
    "> ~~This function takes a Bunch **dataSet** as learning set and uses three sets of **totalWordFreq**, **negWordFreq**, **posWordFreq** and a list **numOfExamples** to calculates related probabilities. Note: It is not completely corrent. Fix later today.~~  \n",
    "> ~~* **evaluation(dataSet:Bunch, prediction:list)**  \n",
    "> This function takes a Bunch **dataSet** and a list **prediction** to generate a set containing true pos\\true neg\\false pos\\false neg\n",
    "> * **loadData(address:str)**~~  \n",
    "> This function takes a string **address** and generate a dict of 3 lists (pos, neg, all) of dict ('comment', 'isPos')  \n",
    "\n",
    "**<February 13, 2019>** Pengnan Fan fixs numOfExistanceStopword and bernoulliNaiveBayes\n",
    "> * **advancedNumOfExistance(dataset:dict of list of dict)**  \n",
    "> This funciton takes a dict of list of dict **dataSet** and calculate the existance of words without stopwords, puctuations, and duplicates.  \n",
    "> * **bernoulliNaiveBayes(dataSet:list of dict, wordExistance:dict of list, size:dict of list)**  \n",
    "> This function takes a list of dict **dataSet** as predicting set and uses a dict of list **wordExistance** and a dict of list **size** to calculates related probabilities.  \n",
    "> * **evaluation(dataSet:list of dict, prediction:list)**  \n",
    "> This function takes a list of dict **dataSet** and a list **prediction** to generate a set containing true pos\\true neg\\false pos\\false neg  \n",
    "> * **Outcome** TP =  11848, TN =  11482, FP =  1018, FN =  652  \n",
    ">> accuracy on train set =  93.32 %  \n",
    ">> accuracy on test set = 50%  \n",
    "> **Possible Reason** Overfitting caused by too many features.  \n",
    "\n",
    "**<February 18, 2019>** Pengnan finished crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Preparation  \n",
    "by Pengnan, Sherry, and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.1 - List of Packages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import math\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import numpy\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import contractions\n",
    "from itertools import groupby\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0.2 - Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Please add your address here as string\n",
    "ADDRESS_TRAIN_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "ADDRESS_TEST_PENGNAN = \"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan Fan\n",
    "# @param address of train set\n",
    "# @return a dict of list of dict\n",
    "def loadData(address):\n",
    "    print(\"Start loading negative set\")\n",
    "    neg = sklearn.datasets.load_files(address, categories={\"neg\"})\n",
    "    print(\"Complete loading negative set\")\n",
    "    print(\"Start loading positive set\")\n",
    "    pos = sklearn.datasets.load_files(address, categories={\"pos\"})\n",
    "    print(\"Complete loading postive set\")\n",
    "    \n",
    "    negSet = list()\n",
    "    count = 0\n",
    "    size = len(neg.data)\n",
    "    for x in neg.data:\n",
    "        negSet.append({\"comment\":x.decode('utf-8'), \"isPos\":0})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Preparing negative set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    posSet = list()\n",
    "    count = 0\n",
    "    size = len(pos.data)\n",
    "    for x in pos.data:\n",
    "        posSet.append({\"comment\":x.decode('utf-8'), \"isPos\":1})\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"Prepare positive set: [%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    output = {'pos':posSet, 'neg':negSet, 'all':posSet+negSet}\n",
    "    print(\"\\nFinish preparing\")\n",
    "    return output\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Path\n",
    "train_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\train\"\n",
    "test_path=\"D:\\\\McGill\\\\19Fall\\\\COMP 551\\\\Projects\\\\Project2\\\\comp-551-imbd-sentiment-classification\\\\test\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads train data to a list\n",
    "# @author\n",
    "# @param class_name: either \"pos\" for loading positive data and otherwise loading negative data\n",
    "# @return data: list of train data\n",
    "def load_train(class_name):\n",
    "    label=0\n",
    "    if class_name is \"pos\":\n",
    "        print(\"Start loading positive data\")\n",
    "        label=1\n",
    "    else:\n",
    "        print(\"Start loading negative data\")\n",
    "    data=[]\n",
    "    count = 0\n",
    "    lenW = 12500\n",
    "    for file in glob.glob(train_path+\"/\"+class_name+\"/*.txt\"):\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),label])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    if class_name is \"pos\":\n",
    "        print(\"\\nComplete loading positive data\")\n",
    "    else:\n",
    "        print(\"\\nComplete loading negative data\")\n",
    "    return data\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This function loads test data to a list\n",
    "# @author\n",
    "# @return data: list of test data\n",
    "def load_test():\n",
    "    data=[]\n",
    "    print(\"Start loading test data\")\n",
    "    count = 0\n",
    "    lenW = 25000\n",
    "    for i in range(0,25000):\n",
    "        file=test_path+\"/\"+str(i)+\".txt\"\n",
    "        f = open(file, \"r\")\n",
    "        data.append([f.read(),i])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete loading test data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 - Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set for prediction\n",
    "# @param wordExistance: dict {'pos' -> pos word existance, 'neg' -> neg word existance, 'all' -> all word existance}\n",
    "# @param wordSet: list of unique words\n",
    "# @param size: dict {'pos' -> pos size, 'neg' -> neg size, 'all' -> all size}\n",
    "# @return prediction\n",
    "def bernoulliNaiveBayes(dataSet, wordExistance, wordSet, size):\n",
    "    pPos = size['pos']/size['all']\n",
    "    pNeg = size['neg']/size['all']\n",
    "    prediction = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start Bernoulli naive Bayes classifying\")\n",
    "    for exp in dataSet:\n",
    "        #pPos_x = numpy.log([pPos])\n",
    "        #pNeg_x = numpy.log([pNeg])\n",
    "        \n",
    "        pred = numpy.log([pPos/nNeg])\n",
    "        \n",
    "        # Calculating P(Y|X)\n",
    "        for word in wordSet:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            \n",
    "            if word in exp:\n",
    "                pos = (wordExistance['pos'][word]+1)/(size['pos']+2)\n",
    "                neg = (wordExistance['neg'][word]+1)/(size['neg']+2)\n",
    "            else:\n",
    "                pos = 1 - ((wordExistance['pos'][word]+1)/(size['pos']+2))\n",
    "                neg = 1 - ((wordExistance['neg'][word]+1)/(size['neg']+2))\n",
    "            \n",
    "            #pPos_x += numpy.log([pos])\n",
    "            #pNeg_x += numpy.log([neg])\n",
    "            pred+=numpy.log([pos])\n",
    "        \n",
    "        # Logistic decision boundary\n",
    "        #log_ratio = pPos_x - pNeg_x\n",
    "        \n",
    "        if pred > 0:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    print(\"\\nComplete Bernoulli naive Bayes classifying\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: train set with label\n",
    "# @param prediction: prediction of each example in the trainSet\n",
    "# @return result: list of results: {TP: true pos, TN: true neg, FP: false pos, FN: false neg}\n",
    "def evaluation(dataSet, prediction):\n",
    "    size = len(prediction)\n",
    "    count = 0\n",
    "    result = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    \n",
    "    print(\"Start evaluating classification\")\n",
    "    for i in range(size):\n",
    "        if prediction[i]==1:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['TP']+=1\n",
    "            else:\n",
    "                result['FP']+=1\n",
    "        else:\n",
    "            if dataSet[i]['isPos']==1:\n",
    "                result['FN']+=1\n",
    "            else:\n",
    "                result['TN']+=1\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(size)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete evaluating classification\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Logistic Regression and Support Vector Machine  \n",
    "by Sherry and Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(X,y_train):\n",
    "    lg=LogisticRegression()\n",
    "    lg.fit(X,y_train)\n",
    "    result = sklearn.model_selection.cross_validate(lg, X, y_train, cv=5, return_train_score=True)\n",
    "    print (\"LR Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"LR Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X,y_train):\n",
    "    clf=SVC(kernel='linear')\n",
    "    clf.fit(X,y_train)\n",
    "    result = sklearn.model_selection.cross_validate(clf, X, y_train, cv=5, return_train_score=True)\n",
    "    print (\"SVM Train Accuracy = \", result['train_score'])\n",
    "    print(\"Avg Train Accuracy = \", numpy.mean(result['train_score']))\n",
    "    print (\"SVM Cross Validation Accuracy = \", result['test_score'])\n",
    "    print(\"Avg Validation Accuracy = \", numpy.mean(result['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecTree(x_train, y_train, x_test, y_test):\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=10,min_samples_leaf=2,max_leaf_nodes=300,min_samples_split=2)\n",
    "    dt.fit(x_train, y_train)\n",
    "    x_v=vectorizer.transform(x_test)\n",
    "    y_pred = dt.predict(x_v)\n",
    "    print (\"Decision Tree Accuracy: \", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
    "    print (sklearn.metrics.confusion_matrix(y_test, y_pred, labels=[1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Feature Extraction Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 - Word Frequency without Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @acknowledgement Yuxiang Ma, for this function is edited based on his in miniproject1\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: word frequency without stopwords\n",
    "def wordsFrequencyStopword(dataSet):\n",
    "    stopwordCount = dict()\n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of positive set\")\n",
    "    \n",
    "    for comment in dataSet['pos']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    posCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of positive set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of negative set\")\n",
    "    \n",
    "    for comment in dataSet['neg']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    negCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of negative set\")\n",
    "    \n",
    "    totalString = str()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting naive word frequency of all set\")\n",
    "    \n",
    "    for comment in dataSet['all']:\n",
    "        totalString = totalString + ' ' + comment['comment'].lower()\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    allCountDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    print(\"\\nComplete counting naive word frequency of all set\")\n",
    "    \n",
    "    stopwordCount = {'pos':posCountDict, 'neg':negCountDict, 'all':allCountDict}\n",
    "    \n",
    "    return stopwordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 - Number of Naive Existance of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of words of num of naive existances\n",
    "def numOfExistanceNaive(dataSet):\n",
    "    naiveCount = dict()\n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of positive set\")\n",
    "    for comment in dataSet['pos']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    posCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of positive set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of negative set\")\n",
    "    for comment in dataSet['neg']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    negCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of negative set\")\n",
    "    \n",
    "    totalComments = []\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of naive word existance of all set\")\n",
    "    for comment in dataSet['all']:\n",
    "        commentSplit = comment['comment'].lower().split(\" \")\n",
    "        wordsToAdd = []\n",
    "        for word in commentSplit:\n",
    "            if word not in wordsToAdd:\n",
    "                wordsToAdd.append(word)\n",
    "        totalComments+=wordsToAdd\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    allCount = Counter(x for x in totalComments)\n",
    "    print(\"\\nComplete counting number of naive word existance of all set\")\n",
    "    \n",
    "    naiveCount = {'pos':posCount, 'neg':negCount, 'all':allCount}\n",
    "    \n",
    "    return naiveCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.1 - Number of Existance of Words without Stopwords, Duplicates, and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords, duplicates, and with stemmer\n",
    "def advancedNumOfExistance(dataSet):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in sent_clean.split():\n",
    "            word = ps.stem(word)\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.2 - With Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @return naiveCount: list of num of existances of words without stopwords and with stemmer\n",
    "def advancedNumOfExistanceWithDuplicates(dataSet):\n",
    "    output = dict()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    eachFreqPos = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_pos = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqPos.append(temp_pos)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    eachFreqNeg = list()\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "                \n",
    "        wordSet+=sent_clean.split()\n",
    "        \n",
    "        temp_neg = Counter(s for s in sent_clean.split() if s not in stopwordsSet)\n",
    "        \n",
    "        eachFreqNeg.append(temp_neg)\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of words without stopwords of existance of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        # processedComment = ([k for k, v in groupby(sent_clean.split())])\n",
    "        \n",
    "        wordSet+=sent_clean.split()\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet if s not in stopwordsSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of words without stopwords of existance of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    eachFreq = eachFreqPos+eachFreqNeg\n",
    "    \n",
    "    output = {'allFreq':countDict, 'individual':eachFreq}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.3 - Number of Existance of N-gram Words without Stopwords, Duplicates and with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set of comments\n",
    "# @param n: used for n-gram\n",
    "# @return countDict: list of num of existances of words without stopwords, duplicates and with stemmer\n",
    "def nGram(dataSet, n):\n",
    "    ps = PorterStemmer()\n",
    "    countDict = dict()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['pos'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    for exp in dataSet['pos']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    posDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of positive set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['neg'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    for exp in dataSet['neg']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    \n",
    "    negDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nComplete counting number of \", n, \"-grams without duplicates of negative set\")\n",
    "    \n",
    "    wordSet = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet['all'])\n",
    "    \n",
    "    print(\"Start counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    for exp in dataSet['all']: \n",
    "        comment = contractions.fix(exp['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        withoutStopwords = list()\n",
    "        for x in sent_clean.split():\n",
    "            if x not in stopwordsSet:\n",
    "                withoutStopwords.append(ps.stem(x))\n",
    "        bi = list(nltk.ngrams(withoutStopwords, n))\n",
    "        processedComment = []\n",
    "        \n",
    "        for word in bi:\n",
    "            if word not in processedComment:\n",
    "                processedComment.append(word)\n",
    "                \n",
    "        wordSet+=processedComment\n",
    "            \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1     \n",
    "    \n",
    "    allDict = Counter(s for s in wordSet)\n",
    "    \n",
    "    print(\"\\nCounting counting number of \", n, \"-grams without duplicates of all set\")\n",
    "    \n",
    "    countDict = {'neg':negDict, 'pos':posDict, 'all':allDict}\n",
    "    \n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3.4 - CountAllWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet: set used to learn\n",
    "# @return output: list of unique words shown in dataSet\n",
    "def allWords(dataSet):\n",
    "    output = list()\n",
    "    lenW = len(dataSet)\n",
    "    count = 0\n",
    "    print(\"Start counting all words\")\n",
    "    for comments in dataSet:\n",
    "        for word in comments:\n",
    "            if word not in output:\n",
    "                output.append(word)\n",
    "                \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete counting all words\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4 - Data Standardization and Clean-up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Pengnan Fan\n",
    "# @param dataSet:comment data need to be proceed\n",
    "# @return proceedData\n",
    "def dataStandardization(dataSet):\n",
    "    proceedData = list()\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    stopwordsSet.add('br')\n",
    "    \n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    \n",
    "    print(\"Start proceeding data\")\n",
    "    for data in dataSet: \n",
    "        comment = contractions.fix(data['comment'].lower())\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        toAdd = list()\n",
    "        for x in processedComment:\n",
    "            if x not in stopwordsSet and x not in toAdd:\n",
    "                toAdd.append(x)\n",
    "                \n",
    "        proceedData.append(toAdd)\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "        \n",
    "    print(\"\\nComplete proceeding data\")\n",
    "    return proceedData\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "# @param dataSet:set of data with comments\n",
    "# @return extendedSentence:set of comments without abbr. i.e. it's -> it is\n",
    "def extendSentences(dataSet):\n",
    "    extenedSentence = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start extending sentences\")\n",
    "    for data in dataSet:\n",
    "        extenedSentence.append([contractions.fix(data[0].lower()),data[1]])\n",
    "        \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete extending sentences\")\n",
    "    return extenedSentence\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Pengnan\n",
    "def removePunctuationAndStopwords(dataSet, additionalStopwords=[]):\n",
    "    clean = list()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    print(\"Start removing punctuation and stopwords\")\n",
    "    for data in dataSet:\n",
    "        comment = data[0].lower()\n",
    "        sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "        sent_clean = comment.translate(sent_map)\n",
    "        processedComment = sent_clean.split()\n",
    "        fixed = str()\n",
    "        for word in processedComment:\n",
    "            if word not in stopwordsSet and word not in additionalStopwords:\n",
    "                fixed = fixed + \" \" + word\n",
    "        clean.append([fixed,data[1]])\n",
    "    \n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing punctuation and stopwords\")\n",
    "    return clean\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def lemmatize(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start lemmatizing\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        clean_text = str()\n",
    "        for word in words:\n",
    "            w = lemm.lemmatize(word)\n",
    "            clean_text += w + \" \"\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete lemmatizing\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "def removeDuplicates(dataSet):\n",
    "    processed = list()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    count = 0\n",
    "    lenW = len(dataSet)\n",
    "    print(\"Start removing duplicates\")\n",
    "    for data in dataSet:\n",
    "        words = word_tokenize(data[0]) # word_tokenize() takes care of stripping too.\n",
    "        noDup = list()\n",
    "        for word in words:\n",
    "            if word not in noDup:\n",
    "                noDup.append(word)\n",
    "                \n",
    "        clean_text = str()\n",
    "        for w in noDup:\n",
    "            clean_text = clean_text + \" \" + w\n",
    "        processed.append([clean_text, data[1]])\n",
    "        # This is for loading bar\n",
    "        sys.stdout.write('\\r')\n",
    "        c = int((float(count) / float(lenW)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(c / 5), c))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        count += 1\n",
    "    print(\"\\nComplete removing duplicates\")\n",
    "    return processed\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author Qifei, Pengnan\n",
    "# @param text: a string of comment\n",
    "# @param custom_stopwords:list of additional stopwords\n",
    "# @return clean_txt: a string of clean-up comment\n",
    "def cleanUp(text, custom_stopwords=[]):\n",
    "    # Initilaise Lemmatizer object:\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Extend words\n",
    "    comment = contractions.fix(text.lower())\n",
    "    sent_map = comment.maketrans(dict.fromkeys(string.punctuation))\n",
    "    sent_clean = comment.translate(sent_map)\n",
    "    \n",
    "    # Load NLTK stopwords:\n",
    "    my_stopwords = stopwords.words('english') + custom_stopwords\n",
    "    \n",
    "    clean_text = ''\n",
    "    \n",
    "    words = word_tokenize(sent_clean) # word_tokenize() takes care of stripping too.\n",
    "    \n",
    "    for word in words:\n",
    "        w = lemm.lemmatize(w)\n",
    "        if w not in my_stopwords and len(w)>2:\n",
    "            clean_text += w + \" \"\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# @author\n",
    "# @param dataset: list of comments\n",
    "def cleanUp_data(dataset):\n",
    "    for i in range(0,len(dataset)):\n",
    "        comment=cleanUp(dataset[i][0])\n",
    "        dataset[i][0]=comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading positive data\n",
      "[=================== ] 99%\n",
      "Complete loading positive data\n",
      "Start loading negative data\n",
      "[=================== ] 99%\n",
      "Complete loading negative data\n",
      "Start loading test data\n",
      "[=================== ] 99%\n",
      "Complete loading test data\n"
     ]
    }
   ],
   "source": [
    "train_data=load_train(\"pos\")+load_train(\"neg\")\n",
    "test_data=load_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - No data processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.87515 0.87805 0.87735 0.87565 0.87645]\n",
      "Avg Train Accuracy =  0.87653\n",
      "LR Cross Validation Accuracy =  [0.8506 0.8454 0.8444 0.844  0.8518]\n",
      "Avg Validation Accuracy =  0.84724\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Train Accuracy =  [0.90865 0.91125 0.9105  0.90965 0.9083 ]\n",
      "Avg Train Accuracy =  0.90967\n",
      "SVM Cross Validation Accuracy =  [0.8674 0.8636 0.857  0.861  0.8724]\n",
      "Avg Validation Accuracy =  0.8642799999999999\n"
     ]
    }
   ],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9329  0.9363  0.93415 0.9335  0.9337 ]\n",
      "Avg Train Accuracy =  0.9341100000000001\n",
      "LR Cross Validation Accuracy =  [0.8456 0.836  0.8378 0.8404 0.8388]\n",
      "Avg Validation Accuracy =  0.83972\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.98035 0.98145 0.9811  0.9785  0.97925]\n",
      "Avg Train Accuracy =  0.9801300000000002\n",
      "LR Cross Validation Accuracy =  [0.7958 0.7972 0.7884 0.7874 0.7808]\n",
      "Avg Validation Accuracy =  0.7899200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in train_data]\n",
    "train_y=[comment[1] for comment in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data processing Test, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9085  0.9082  0.9079  0.9072  0.90545]\n",
      "Avg Train Accuracy =  0.9074500000000001\n",
      "LR Cross Validation Accuracy =  [0.8556 0.8474 0.8456 0.8456 0.854 ]\n",
      "Avg Validation Accuracy =  0.84964\n"
     ]
    }
   ],
   "source": [
    "print(\"No data processing Test, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending Abbr. and All Lower Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extending sentences\n",
      "[=================== ] 99%\n",
      "Complete extending sentences\n"
     ]
    }
   ],
   "source": [
    "extended = extendSentences(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Unigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.87455 0.87815 0.8771  0.87505 0.87445]\n",
      "Avg Train Accuracy =  0.87586\n",
      "LR Cross Validation Accuracy =  [0.85   0.844  0.8422 0.845  0.8522]\n",
      "Avg Validation Accuracy =  0.8466799999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Unigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Bigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9297  0.93135 0.9309  0.92865 0.9294 ]\n",
      "Avg Train Accuracy =  0.93\n",
      "LR Cross Validation Accuracy =  [0.8438 0.8364 0.8382 0.8398 0.836 ]\n",
      "Avg Validation Accuracy =  0.83884\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Bigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Trigram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9775  0.97865 0.9776  0.9758  0.97615]\n",
      "Avg Train Accuracy =  0.97714\n",
      "LR Cross Validation Accuracy =  [0.804  0.7966 0.7928 0.7926 0.7892]\n",
      "Avg Validation Accuracy =  0.79504\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Trigram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 All-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extended]\n",
    "train_y=[comment[1] for comment in extended]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending Abbr. and Allgram Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.906   0.90595 0.9065  0.906   0.9046 ]\n",
      "Avg Train Accuracy =  0.90581\n",
      "LR Cross Validation Accuracy =  [0.853  0.8476 0.847  0.8456 0.8534]\n",
      "Avg Validation Accuracy =  0.84932\n"
     ]
    }
   ],
   "source": [
    "print(\"Extending Abbr. and Allgram Test\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Extended Abbr. Without Stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing punctuation and stopwords\n",
      "[=================== ] 99%\n",
      "Complete removing punctuation and stopwords\n"
     ]
    }
   ],
   "source": [
    "extend_withoutStopwordsAndPunctions = removePunctuationAndStopwords(extended, additionalStopwords=['br'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9114  0.91475 0.91325 0.91255 0.9107 ]\n",
      "Avg Train Accuracy =  0.91253\n",
      "LR Cross Validation Accuracy =  [0.8634 0.8574 0.8526 0.8584 0.8676]\n",
      "Avg Validation Accuracy =  0.8598800000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9824  0.9836  0.9823  0.9816  0.98215]\n",
      "Avg Train Accuracy =  0.98241\n",
      "LR Cross Validation Accuracy =  [0.7954 0.7806 0.7828 0.797  0.7932]\n",
      "Avg Validation Accuracy =  0.7898000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9996  0.99965 0.9995  0.9996  0.9995 ]\n",
      "Avg Train Accuracy =  0.9995700000000001\n",
      "LR Cross Validation Accuracy =  [0.683  0.6686 0.6474 0.6776 0.6642]\n",
      "Avg Validation Accuracy =  0.66816\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in extend_withoutStopwordsAndPunctions]\n",
    "train_y=[comment[1] for comment in extend_withoutStopwordsAndPunctions]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations, Allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93305 0.935   0.93425 0.9334  0.9319 ]\n",
      "Avg Train Accuracy =  0.93352\n",
      "LR Cross Validation Accuracy =  [0.8582 0.8536 0.8448 0.8524 0.861 ]\n",
      "Avg Validation Accuracy =  0.8539999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations, Allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Extended Abbr. Without Stopwords and Punctuations and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing\n",
      "[=================== ] 99%\n",
      "Complete lemmatizing\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize(extend_withoutStopwordsAndPunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.90855 0.91175 0.91045 0.90895 0.9082 ]\n",
      "Avg Train Accuracy =  0.90958\n",
      "LR Cross Validation Accuracy =  [0.8626 0.8558 0.85   0.856  0.8606]\n",
      "Avg Validation Accuracy =  0.857\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.97755 0.9785  0.977   0.97685 0.97875]\n",
      "Avg Train Accuracy =  0.97773\n",
      "LR Cross Validation Accuracy =  [0.7924 0.77   0.7716 0.7922 0.7932]\n",
      "Avg Validation Accuracy =  0.78388\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.999   0.99885 0.99895 0.99915 0.9988 ]\n",
      "Avg Train Accuracy =  0.9989500000000001\n",
      "LR Cross Validation Accuracy =  [0.663  0.6524 0.64   0.666  0.6526]\n",
      "Avg Validation Accuracy =  0.6548\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, Trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in lemmatized]\n",
    "train_y=[comment[1] for comment in lemmatized]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.93105 0.9331  0.93175 0.9306  0.9293 ]\n",
      "Avg Train Accuracy =  0.93116\n",
      "LR Cross Validation Accuracy =  [0.855  0.85   0.8448 0.8508 0.8586]\n",
      "Avg Validation Accuracy =  0.8518399999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords and Punctuations and Lemmatized, allgram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing duplicates\n",
      "[=================== ] 99%\n",
      "Complete removing duplicates\n"
     ]
    }
   ],
   "source": [
    "noDuplicates = removeDuplicates(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9131  0.91605 0.9146  0.9125  0.91075]\n",
      "Avg Train Accuracy =  0.9134\n",
      "LR Cross Validation Accuracy =  [0.8662 0.8652 0.8654 0.8668 0.8724]\n",
      "Avg Validation Accuracy =  0.8672000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, unigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9882  0.9881  0.98885 0.9876  0.9883 ]\n",
      "Avg Train Accuracy =  0.9882099999999999\n",
      "LR Cross Validation Accuracy =  [0.7654 0.7532 0.7556 0.761  0.7682]\n",
      "Avg Validation Accuracy =  0.76068\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, bigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.99935 0.9992  0.99925 0.99955 0.9994 ]\n",
      "Avg Train Accuracy =  0.99935\n",
      "LR Cross Validation Accuracy =  [0.637  0.6176 0.6068 0.628  0.6316]\n",
      "Avg Validation Accuracy =  0.6242\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, trigram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Allgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[comment[0] for comment in noDuplicates]\n",
    "train_y=[comment[1] for comment in noDuplicates]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_x)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train Accuracy =  [0.9381  0.9396  0.93935 0.9377  0.93645]\n",
      "Avg Train Accuracy =  0.9382400000000001\n",
      "LR Cross Validation Accuracy =  [0.859  0.8566 0.8578 0.8598 0.8624]\n",
      "Avg Validation Accuracy =  0.8591200000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended Abbr. Without Stopwords, Punctuations and duplicates, and Lemmatized, alligram\")\n",
    "LogReg(X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM(X, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
